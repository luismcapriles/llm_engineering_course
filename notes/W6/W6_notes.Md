Here is your text converted into Markdown format:  

```markdown
# Training a Model  

## Requires Data  
Here are some available resources where to get datasets:  
- Your own proprietary data  
- [Kaggle](https://www.kaggle.com/)  
- [HuggingFace datasets](https://huggingface.co/datasets)  
- **Synthetic data** (fictitious data generated by a model) â€“ recommended in some cases  
- Specific companies do it â†’ [scale.com](https://scale.com/)  

## Data Stages (as seen in the course on LLM engineering)  
1. **Investigate** â€“ Understanding how well populated the dataset is and its quality  
2. **Parse** â€“ Formatting the data for easy processing  
3. **Visualize** â€“ Examining distributions using graphs or visual tools  
4. **Assess Data Quality**  
5. **Curate** â€“ Crafting the dataset, filtering out unnecessary elements, and addressing imbalance  
6. **Save**  

---

## Use Case: Training a Model to Predict Item Prices Based on Descriptions  

We use a dataset from HuggingFace related to Amazon product reviews:  
ðŸ”— [Amazon Reviews 2023 Dataset](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023)  
ðŸ”— [Folder with all product datasets](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/tree/main/raw/meta_categories)  

---

## **1. Investigate**  

We started by retrieving the dataset:  

```python
from huggingface_hub import login
from datasets import load_dataset, Dataset, DatasetDict
import os

# Log in to HuggingFace
hf_token = os.environ['HF_TOKEN']
login(hf_token, add_to_git_credential=True)

# Load in our dataset
dataset = load_dataset("McAuley-Lab/Amazon-Reviews-2023", "raw_meta_Appliances", split="full", trust_remote_code=True)

# Checking how many items there are
print(f"Number of Appliances: {len(dataset):,}")
```

**Output:**  
```
Number of Appliances: 94,327
```

---

## **2. Parsing the Data**  

Investigate a particular data point:  

```python
datapoint = dataset[0]
print(datapoint["title"])
print(datapoint["description"])
print(datapoint["features"])
print(datapoint["details"])
print(datapoint["price"])
```

We noticed that some products **do not have a price value**. This is an issue since we need the price for training. We need to filter out items without prices.  

**How many have prices?**  

```python
prices = 0
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            prices += 1
    except ValueError:
        pass

print(f"There are {prices:,} items with prices, which is {prices/len(dataset)*100:,.1f}%")
```

**Output:**  
```
There are 46,726 items with prices, which is 49.5%
```

### Checking the Length of Each Item Description  

```python
prices = []
lengths = []
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            prices.append(price)
            contents = datapoint["title"] + str(datapoint["description"]) + str(datapoint["features"]) + str(datapoint["details"])
            lengths.append(len(contents))
    except ValueError:
        pass
```

---

## **3. Visualizing the Data**  

### Distribution of Item Content Lengths  

We use Matplotlib for visualization:  

```python
import matplotlib.pyplot as plt
%matplotlib inline

# Plot the distribution of item content lengths
plt.figure(figsize=(15, 6))
plt.title(f"Lengths: Avg {sum(lengths)/len(lengths):,.0f} and highest {max(lengths):,}\n")
plt.xlabel('Length (chars)')
plt.ylabel('Count')
plt.hist(lengths, rwidth=0.7, color="lightblue", bins=range(0, 6000, 100))
plt.show()
```

### Distribution of Prices  

```python
# Plot the distribution of prices
plt.figure(figsize=(15, 6))
plt.title(f"Prices: Avg {sum(prices)/len(prices):,.2f} and highest {max(prices):,}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="orange", bins=range(0, 1000, 10))
plt.show()
```

**Observations:**  
- Some items have **very high prices**, which could impact the results.  

---

## **4. Curating the Data**  

### Considerations:  
- **Cost & Memory Factor:**  
  - The number of tokens affects training costs for Closed-Source models.  
  - **More tokens â†’ More memory required â†’ Harder to train.**  

Since we're working with **Closed-Source models**, we must be mindful of **token usage**.  
- We will **limit the number of tokens** used to train the model.  
- We need to **curate** each item to fit within a certain token limit.  

### **Curating Strategy:**  
- Select items **priced between $1 and $999**  
- Create **Item instances** that:  
  - **Truncate text** to fit within **180 tokens** using the right tokenizer  
  - **Generate a prompt** for training  

**Tokenizer Used:** Meta-Llama-3.1-8B  
- If an item doesnâ€™t have sufficient characters, it is **rejected**  

### **Why 180 Tokens?**  
This is a **hyperparameter** â€“ determined through **trial and error**.  
- Needs to be **large enough** to capture useful price-related information.  
- Needs to be **small enough** for efficient training.  
- Mimics **inference-time inputs** (short descriptions of 1-2 sentences).  

ðŸ’¡ *If you experiment with different token limits, you might get better results!*  

---

## **5. A Potential Issue in the `Item` Code**  

<details>
<summary><strong>Issue: Truncated Words and Sentences</strong></summary>

### Problem Breakdown:  
1. **Chopped Sentences & Words**  
   - The code truncates text based on character count (`contents = contents[:CEILING_CHARS]`).  
   - Then tokenizes the text.  
   - Then further trims based on token count (`tokens = tokens[:MAX_TOKENS]`).  
   - Finally, it decodes back into text.  

2. **Misleading Content for the Model**  
   - Truncation can **cut sentences mid-way**, leading to **loss of meaning**.  
   - Example:  
     ```
     "This product is highly durable and..."
     ```
     could become  
     ```
     "This product is highly dur..."
     ```
   - **Tokenization can lose meaning** if a word is cut off (e.g., `"application"` â†’ `"applicat"`).  

3. **Impact on Model Training**  
   - **Reduced Accuracy**: Model learns from incomplete/misleading data.  
   - **Bias**: Some product descriptions might be disproportionately affected.  
   - **Instability**: Model might struggle to converge.  

### **Possible Solutions:**  
- **Sentence-Based Truncation**: Cut at sentence boundaries.  
- **Word-Based Truncation Before Tokenization**: Avoid splitting words.  
- **Summarization**: Use NLP techniques to shorten descriptions meaningfully.  
- **Increase `MAX_TOKEN` and `CEILING_CHARS`**: If feasible, raise limits to reduce unwanted truncation.  

---

**Note from Ed:**  
> "Hey Luis - `CEILING_CHARS` was meant to improve processing efficiency, not to truncate important data. If it's cutting meaningful content, that's a bug! A quick fix: triple `CEILING_CHARS`. The model should handle tokenized fragments, but real truncation affecting `MAX_TOKENS` is a problem. Let me know!"  

</details>
```  
