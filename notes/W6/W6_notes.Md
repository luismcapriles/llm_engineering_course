# Training a Model  

## Requires Data  
Here are some available resources where to get datasets:  
- Your own proprietary data  
- [Kaggle](https://www.kaggle.com/)  
- [HuggingFace datasets](https://huggingface.co/datasets)  
- **Synthetic data** (fictitious data generated by a model) â€“ recommended in some cases  
- Specific companies do it â†’ [scale.com](https://scale.com/)  

## Data Stages (as seen in the course on LLM engineering)  
1. **Investigate** â€“ Understanding how well populated the dataset is and its quality  
2. **Parse** â€“ Formatting the data for easy processing  
3. **Visualize** â€“ Examining distributions using graphs or visual tools  
4. **Assess Data Quality**  
5. **Curate** â€“ Crafting the dataset, filtering out unnecessary elements, and addressing imbalance  
6. **Save**  

---

## Use Case: Training a Model to Predict Item Prices Based on Descriptions  

We use a dataset from HuggingFace related to Amazon product reviews:  
ðŸ”— [Amazon Reviews 2023 Dataset](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023)  
ðŸ”— [Folder with all product datasets](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/tree/main/raw/meta_categories)  

---

## **1. Investigate**  

We started by retrieving the dataset:  

```python
from huggingface_hub import login
from datasets import load_dataset, Dataset, DatasetDict
import os

# Log in to HuggingFace
hf_token = os.environ['HF_TOKEN']
login(hf_token, add_to_git_credential=True)

# Load in our dataset
dataset = load_dataset("McAuley-Lab/Amazon-Reviews-2023", "raw_meta_Appliances", split="full", trust_remote_code=True)

# Checking how many items there are
print(f"Number of Appliances: {len(dataset):,}")
```

**Output:**  
```
Number of Appliances: 94,327
```

---

## **2. Parsing the Data**  

Investigate a particular data point:  

```python
datapoint = dataset[0]
print(datapoint["title"])
print(datapoint["description"])
print(datapoint["features"])
print(datapoint["details"])
print(datapoint["price"])
```

We noticed that some products **do not have a price value**. This is an issue since we need the price for training. We need to filter out items without prices.  

**How many have prices?**  

```python
prices = 0
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            prices += 1
    except ValueError:
        pass

print(f"There are {prices:,} items with prices, which is {prices/len(dataset)*100:,.1f}%")
```

**Output:**  
```
There are 46,726 items with prices, which is 49.5%
```

### Checking the Length of Each Item Description  

```python
prices = []
lengths = []
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            prices.append(price)
            contents = datapoint["title"] + str(datapoint["description"]) + str(datapoint["features"]) + str(datapoint["details"])
            lengths.append(len(contents))
    except ValueError:
        pass
```

---

## **3. Visualizing the Data**  

### Distribution of Item Content Lengths  

We use Matplotlib for visualization:  

```python
import matplotlib.pyplot as plt
%matplotlib inline

# Plot the distribution of item content lengths
plt.figure(figsize=(15, 6))
plt.title(f"Lengths: Avg {sum(lengths)/len(lengths):,.0f} and highest {max(lengths):,}\n")
plt.xlabel('Length (chars)')
plt.ylabel('Count')
plt.hist(lengths, rwidth=0.7, color="lightblue", bins=range(0, 6000, 100))
plt.show()
```
![item_length](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img01_item_detail_lenght.png)

### Distribution of Prices  

```python
# Plot the distribution of prices
plt.figure(figsize=(15, 6))
plt.title(f"Prices: Avg {sum(prices)/len(prices):,.2f} and highest {max(prices):,}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="orange", bins=range(0, 1000, 10))
plt.show()
```
![price_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img02_item_ave_price.png)

**Observations:**  
- Some items have **very high prices**, which could impact the results.  

---

## **4. Curating the Data**  

### Considerations:  
- **Cost & Memory Factor:**  
  - The number of tokens affects training costs for Closed-Source models.  
  - **More tokens â†’ More memory required â†’ Harder to train.**  

Since we're working with **Closed-Source models**, we must be mindful of **token usage**.  
- We will **limit the number of tokens** used to train the model.  
- We need to **curate** each item to fit within a certain token limit.  

### **Curating Strategy:**  
- Select items **priced between $1 and $999**  
- Create **Item instances** that:  
  - **Truncate text** to fit within **180 tokens** using the right tokenizer  
  - **Generate a prompt** for training  

**Tokenizer Used:** Meta-Llama-3.1-8B  
- If an item doesnâ€™t have sufficient characters, it is **rejected**  

![item_class](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img03_item_class.png)
### **Why 180 Tokens?**  
This is a **hyperparameter** â€“ determined through **trial and error**.  
- Needs to be **large enough** to capture useful price-related information.  
- Needs to be **small enough** for efficient training.  
- Mimics **inference-time inputs** (short descriptions of 1-2 sentences).  

ðŸ’¡ *If you experiment with different token limits, you might get better results!*  

---

###  **4.1. A Potential Issue in the `Item` Code**  

<details>
<summary><strong>Issue: Truncated Words and Sentences</strong></summary>

### Problem Breakdown:  
1. **Chopped Sentences & Words**  
   - The code truncates text based on character count (`contents = contents[:CEILING_CHARS]`).  
   - Then tokenizes the text.  
   - Then further trims based on token count (`tokens = tokens[:MAX_TOKENS]`).  
   - Finally, it decodes back into text.  

2. **Misleading Content for the Model**  
   - Truncation can **cut sentences mid-way**, leading to **loss of meaning**.  
   - Example:  
     ```
     "This product is highly durable and..."
     ```
     could become  
     ```
     "This product is highly dur..."
     ```
   - **Tokenization can lose meaning** if a word is cut off (e.g., `"application"` â†’ `"applicat"`).  

3. **Impact on Model Training**  
   - **Reduced Accuracy**: Model learns from incomplete/misleading data.  
   - **Bias**: Some product descriptions might be disproportionately affected.  
   - **Instability**: Model might struggle to converge.  

### **Possible Solutions:**  
- **Sentence-Based Truncation**: Cut at sentence boundaries.  
- **Word-Based Truncation Before Tokenization**: Avoid splitting words.  
- **Summarization**: Use NLP techniques to shorten descriptions meaningfully.  
- **Increase `MAX_TOKEN` and `CEILING_CHARS`**: If feasible, raise limits to reduce unwanted truncation.  

---

**Note from Ed:**  
> "Hey Luis - `CEILING_CHARS` was meant to improve processing efficiency, not to truncate important data. If it's cutting meaningful content, that's a bug! A quick fix: triple `CEILING_CHARS`. The model should handle tokenized fragments, but real truncation affecting `MAX_TOKENS` is a problem. Let me know!"  

</details>

## Creating Item Objects for Each Data Point with a Price

```python
# Create an Item object for each with a price
items = []
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            item = Item(datapoint, price) # creating an Item 
            if item.include:
                items.append(item)
    except ValueError as e:
        pass

print(f"There are {len(items):,} items")
```

**Output:**
```
There are 29,191 items
```

### Looking at the First Item
```python
# Look at the first item
items[0]
```
**Output:**
```
<WD12X10327 Rack Roller and stud assembly Kit (4 Pack) by AMI PARTS Replaces AP4980629 PS3486910 1811003 = $8.99>
```

### Investigating the Prompt Used During Training

```python
# Investigate the prompt that will be used during training - the model learns to complete this
print(items[1].prompt)
```
**Output:**
```
How much does this cost to the nearest dollar?

Door Pivot Block - Compatible Kenmore KitchenAid Maytag Whirlpool Refrigerator - Replaces - Quick DIY Repair Solution
Pivot Block For Vernicle Mullion Strip On Door - A high-quality exact equivalent for part numbers and Compatibility with major brands - Door Guide is compatible with Whirlpool, Amana, Dacor, Gaggenau, Hardwick, Jenn-Air, Kenmore, KitchenAid, and Maytag. Quick DIY repair - Refrigerator Door Guide Pivot Block Replacement will help if your appliance door doesn't open or close. Wear work gloves to protect your hands during the repair process. Attentive support - If you are uncertain about whether the block fits your refrigerator, we will help. We generally put forth a valiant effort to guarantee you are totally

Price is $17.00
```

### Investigating the Prompt Used During Testing

```python
# Investigate the prompt that will be used during testing - the model has to complete this
print(items[0].test_prompt())
```
**Output:**
```
How much does this cost to the nearest dollar?

Rack Roller and stud assembly Kit (4 Pack) by AMI PARTS Replaces
PARTS NUMBER The dishwasher top rack wheels and stud assembly Kit ï¼ˆ4 pcsï¼‰ SCOPE OF APPLICATION The dishwasher works with most top name brands,If you are not sure if part is correct, ask us in Customer questions & answers section or visiting the AMI PARTS storefront.Weâ€™re happy to help ensure you select the correct part for your Rack Roller and stud REPLACES PART FIXES SYMPTOMS Door wonâ€™t close | Not cleaning dishes properly | Noisy | Door latch failure QUALITY WARRANTY The replacement part is made from durable high quality material and well-tested by manufacturer.For any reason youâ€™re not satisfied,you can ask for a replacement or full refund Brand Name AMI PARTS, Model

Price is $
```

## Visualizing Token Distribution After Filtering

```python
# Plot the distribution of token counts
tokens = [item.token_count for item in items]
plt.figure(figsize=(15, 6))
plt.title(f"Token counts: Avg {sum(tokens)/len(tokens):,.1f} and highest {max(tokens):,}\n")
plt.xlabel('Length (tokens)')
plt.ylabel('Count')
plt.hist(tokens, rwidth=0.7, color="green", bins=range(0, 300, 10))
plt.show()
```

## Analyzing the New Price Distribution

```python
# Plot the distribution of prices
prices = [item.price for item in items]
plt.figure(figsize=(15, 6))
plt.title(f"Prices: Avg {sum(prices)/len(prices):,.1f} and highest {max(prices):,}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="purple", bins=range(0, 1000, 10))
plt.show()
```
