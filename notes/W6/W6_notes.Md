# Training a Model  

## Requires Data  
Here are some available resources where to get datasets:  
- Your own proprietary data  
- [Kaggle](https://www.kaggle.com/)  
- [HuggingFace datasets](https://huggingface.co/datasets)  
- **Synthetic data** (fictitious data generated by a model) – recommended in some cases  
- Specific companies do it → [scale.com](https://scale.com/)  

## Data Stages (as seen in the course on LLM engineering)  
1. **Investigate** – Understanding how well populated the dataset is and its quality  
2. **Parse** – Formatting the data for easy processing  
3. **Visualize** – Examining distributions using graphs or visual tools  
4. **Assess Data Quality**  
5. **Curate** – Crafting the dataset, filtering out unnecessary elements, and addressing imbalance  
6. **Save**  

---

## Use Case: Training a Model to Predict Item Prices Based on Descriptions  

We use a dataset from HuggingFace related to Amazon product reviews:  
🔗 [Amazon Reviews 2023 Dataset](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023)  
🔗 [Folder with all product datasets](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/tree/main/raw/meta_categories)  

---

## **1. Investigate**  

We started by retrieving the dataset:  

```python
from huggingface_hub import login
from datasets import load_dataset, Dataset, DatasetDict
import os

# Log in to HuggingFace
hf_token = os.environ['HF_TOKEN']
login(hf_token, add_to_git_credential=True)

# Load in our dataset
dataset = load_dataset("McAuley-Lab/Amazon-Reviews-2023", "raw_meta_Appliances", split="full", trust_remote_code=True)

# Checking how many items there are
print(f"Number of Appliances: {len(dataset):,}")
```

**Output:**  
```
Number of Appliances: 94,327
```

---

## **2. Parsing the Data**  

Investigate a particular data point:  

```python
datapoint = dataset[0]
print(datapoint["title"])
print(datapoint["description"])
print(datapoint["features"])
print(datapoint["details"])
print(datapoint["price"])
```

We noticed that some products **do not have a price value**. This is an issue since we need the price for training. We need to filter out items without prices.  

**How many have prices?**  

```python
prices = 0
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            prices += 1
    except ValueError:
        pass

print(f"There are {prices:,} items with prices, which is {prices/len(dataset)*100:,.1f}%")
```

**Output:**  
```
There are 46,726 items with prices, which is 49.5%
```

### Checking the Length of Each Item Description  

```python
prices = []
lengths = []
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            prices.append(price)
            contents = datapoint["title"] + str(datapoint["description"]) + str(datapoint["features"]) + str(datapoint["details"])
            lengths.append(len(contents))
    except ValueError:
        pass
```

---

## **3. Visualizing the Data**  

### Distribution of Item Content Lengths  

We use Matplotlib for visualization:  

```python
import matplotlib.pyplot as plt
%matplotlib inline

# Plot the distribution of item content lengths
plt.figure(figsize=(15, 6))
plt.title(f"Lengths: Avg {sum(lengths)/len(lengths):,.0f} and highest {max(lengths):,}\n")
plt.xlabel('Length (chars)')
plt.ylabel('Count')
plt.hist(lengths, rwidth=0.7, color="lightblue", bins=range(0, 6000, 100))
plt.show()
```
![item_length](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img01_item_detail_lenght.png)

### Distribution of Prices  

```python
# Plot the distribution of prices
plt.figure(figsize=(15, 6))
plt.title(f"Prices: Avg {sum(prices)/len(prices):,.2f} and highest {max(prices):,}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="orange", bins=range(0, 1000, 10))
plt.show()
```
![price_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img02_item_ave_price.png)

**Observations:**  
- Some items have **very high prices**, which could impact the results.  

---

## **4. Curating the Data**  

## Data Curation Part 1
### Considerations:  
- **Cost & Memory Factor:**  
  - The number of tokens affects training costs for Closed-Source models.  
  - **More tokens → More memory required → Harder to train.**  

Since we're working with **Closed-Source models**, we must be mindful of **token usage**.  
- We will **limit the number of tokens** used to train the model.  
- We need to **curate** each item to fit within a certain token limit.  

### **Curating Strategy:**  
- Select items **priced between $1 and $999**  
- Create **Item instances** that:  
  - **Truncate text** to fit within **180 tokens** using the right tokenizer  
  - **Generate a prompt** for training  

**Tokenizer Used:** Meta-Llama-3.1-8B  
- If an item doesn’t have sufficient characters, it is **rejected**  

![item_class](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img03_item_class.png)
### **Why 180 Tokens?**  
This is a **hyperparameter** – determined through **trial and error**.  
- Needs to be **large enough** to capture useful price-related information.  
- Needs to be **small enough** for efficient training.  
- Mimics **inference-time inputs** (short descriptions of 1-2 sentences).  

💡 *If you experiment with different token limits, you might get better results!*  

---

###  **4.1. A Potential Issue in the `Item` Code**  

<details>
<summary><strong>Issue: Truncated Words and Sentences</strong></summary>

### Problem Breakdown:  
1. **Chopped Sentences & Words**  
   - The code truncates text based on character count (`contents = contents[:CEILING_CHARS]`).  
   - Then tokenizes the text.  
   - Then further trims based on token count (`tokens = tokens[:MAX_TOKENS]`).  
   - Finally, it decodes back into text.  

2. **Misleading Content for the Model**  
   - Truncation can **cut sentences mid-way**, leading to **loss of meaning**.  
   - Example:  
     ```
     "This product is highly durable and..."
     ```
     could become  
     ```
     "This product is highly dur..."
     ```
   - **Tokenization can lose meaning** if a word is cut off (e.g., `"application"` → `"applicat"`).  

3. **Impact on Model Training**  
   - **Reduced Accuracy**: Model learns from incomplete/misleading data.  
   - **Bias**: Some product descriptions might be disproportionately affected.  
   - **Instability**: Model might struggle to converge.  

### **Possible Solutions:**  
- **Sentence-Based Truncation**: Cut at sentence boundaries.  
- **Word-Based Truncation Before Tokenization**: Avoid splitting words.  
- **Summarization**: Use NLP techniques to shorten descriptions meaningfully.  
- **Increase `MAX_TOKEN` and `CEILING_CHARS`**: If feasible, raise limits to reduce unwanted truncation.  

---

**Note from Ed:**  
> "Hey Luis - `CEILING_CHARS` was meant to improve processing efficiency, not to truncate important data. If it's cutting meaningful content, that's a bug! A quick fix: triple `CEILING_CHARS`. The model should handle tokenized fragments, but real truncation affecting `MAX_TOKENS` is a problem. Let me know!"  

</details>

## Creating Item Objects for Each Data Point with a Price

```python
# Create an Item object for each with a price
items = []
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            item = Item(datapoint, price) # creating an Item 
            if item.include:
                items.append(item)
    except ValueError as e:
        pass

print(f"There are {len(items):,} items")
```

**Output:**
```
There are 29,191 items
```

### Looking at the First Item
```python
# Look at the first item
items[0]
```
**Output:**
```
<WD12X10327 Rack Roller and stud assembly Kit (4 Pack) by AMI PARTS Replaces AP4980629 PS3486910 1811003 = $8.99>
```

### Investigating the Prompt Used During Training

```python
# Investigate the prompt that will be used during training - the model learns to complete this
print(items[1].prompt)
```
**Output:**
```
How much does this cost to the nearest dollar?

Door Pivot Block - Compatible Kenmore KitchenAid Maytag Whirlpool Refrigerator - Replaces - Quick DIY Repair Solution
Pivot Block For Vernicle Mullion Strip On Door - A high-quality exact equivalent for part numbers and Compatibility with major brands - Door Guide is compatible with Whirlpool, Amana, Dacor, Gaggenau, Hardwick, Jenn-Air, Kenmore, KitchenAid, and Maytag. Quick DIY repair - Refrigerator Door Guide Pivot Block Replacement will help if your appliance door doesn't open or close. Wear work gloves to protect your hands during the repair process. Attentive support - If you are uncertain about whether the block fits your refrigerator, we will help. We generally put forth a valiant effort to guarantee you are totally

Price is $17.00
```

### Investigating the Prompt Used During Testing

```python
# Investigate the prompt that will be used during testing - the model has to complete this
print(items[0].test_prompt())
```
**Output:**
```
How much does this cost to the nearest dollar?

Rack Roller and stud assembly Kit (4 Pack) by AMI PARTS Replaces
PARTS NUMBER The dishwasher top rack wheels and stud assembly Kit （4 pcs） SCOPE OF APPLICATION The dishwasher works with most top name brands,If you are not sure if part is correct, ask us in Customer questions & answers section or visiting the AMI PARTS storefront.We’re happy to help ensure you select the correct part for your Rack Roller and stud REPLACES PART FIXES SYMPTOMS Door won’t close | Not cleaning dishes properly | Noisy | Door latch failure QUALITY WARRANTY The replacement part is made from durable high quality material and well-tested by manufacturer.For any reason you’re not satisfied,you can ask for a replacement or full refund Brand Name AMI PARTS, Model

Price is $
```

## Visualizing Token Distribution After Filtering

```python
# Plot the distribution of token counts
tokens = [item.token_count for item in items]
plt.figure(figsize=(15, 6))
plt.title(f"Token counts: Avg {sum(tokens)/len(tokens):,.1f} and highest {max(tokens):,}\n")
plt.xlabel('Length (tokens)')
plt.ylabel('Count')
plt.hist(tokens, rwidth=0.7, color="green", bins=range(0, 300, 10))
plt.show()
```
![token_length_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img04_token_length_dist.png)

## Analyzing the New Price Distribution

```python
# Plot the distribution of prices
prices = [item.price for item in items]
plt.figure(figsize=(15, 6))
plt.title(f"Prices: Avg {sum(prices)/len(prices):,.1f} and highest {max(prices):,}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="purple", bins=range(0, 1000, 10))
plt.show()
```
![new_price_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img05_new_item_ave_price_after_180_Token.png)

## Sidenote
If you like the variety of colors that matplotlib can use in its charts, you should bookmark this:
https://matplotlib.org/stable/gallery/color/named_colors.html

## Data Curation Part 2

Today we'll extend our dataset to a greater coverage, and craft it into an excellent dataset for training.  
Data curation can seem less exciting than other things we work on, but it's a crucial part of the LLM engineers' responsibility and an important craft to hone, so that you can build your own commercial solutions with high quality datasets.

<detail><summary><strong>Important Note - read me first please </strong></summary>

We are about to craft a massive dataset of 400,000 items covering multiple types of product. In Week 7 we will be using this data to train our own model. It's a pretty big dataset, and depending on the GPU you select, training could take 20+ hours. It will be really good fun, but it could cost a few dollars in compute units.

As an alternative, if you want to keep things quick & low cost, you can work with a smaller dataset focused only on Home Appliances. You'll be able to cover the same learning points; the results will be good -- not quite as good as the full dataset, but still pretty amazing! If you'd prefer to do this, I've set up an alternative jupyter notebook in this folder called `lite.ipynb` that you should use in place of this one.

Also, if you'd prefer, you can shortcut running all this data curation by downloading the pickle files that we save in the last cell. The pickle files are available here: https://drive.google.com/drive/folders/1f_IZGybvs9o0J5sb3xmtTEQB3BXllzrW
</detail>

```python
# imports
import os
import random
from dotenv import load_dotenv
from huggingface_hub import login
from datasets import load_dataset, Dataset, DatasetDict
from items import Item
from loaders import ItemLoader
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
import numpy as np
import pickle

# environment

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')
os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')

# Log in to HuggingFace

hf_token = os.environ['HF_TOKEN']
login(hf_token, add_to_git_credential=True)

%matplotlib inline
```
## The ItemLoader code
Look in loaders.py - there's some useful code to make life easier for us

Here is your content formatted in **Markdown**:

```markdown
```python
# Load one dataset at a time -> In this case only the Appliances
items = ItemLoader("Appliances").load()
```

## Now to SCALE UP
Let's look at all datasets of all the items that you might find in a large home retail store - electrical, electronic, office and related, but not clothes / beauty / books.

```python
dataset_names = [
    "Automotive",
    "Electronics",
    "Office_Products",
    "Tools_and_Home_Improvement",
    "Cell_Phones_and_Accessories",
    "Toys_and_Games",
    "Appliances",
    "Musical_Instruments",
]
```

# Now, time for a coffee break!!  
### By the way, I put the biggest datasets first.. it gets faster.

```python
items = []
for dataset_name in dataset_names:
    loader = ItemLoader(dataset_name)
    items.extend(loader.load(workers=10))
```

```python
print(f"A grand total of {len(items):,} items")
```

**Output:**  
*A grand total of 2,811,408 items*

---

[Note]: Recommendation of HW setting for handling heavy tasks like this.  
In most cases, for an **LLM engineer**, a machine with **50GB drive space and 16GB memory** is sufficient; you can survive on less, and more is nice but a luxury.  

Personally, I'm an advocate for using **cloud servers** for GPUs. Some people like to invest in hefty GPU boxes for local training, and I get that—it's satisfying to run everything locally. But you could end up dropping **$4,000** for a high-end GPU box, and in **12 months** it might be obsolete. 
I personally prefer to run on **the cloud**, and for **under $20 a month**, I get access to a lot of power.  

---

## Plot the distribution of prices
```python
prices = [item.price for item in items]
plt.figure(figsize=(15, 6))
plt.title(f"Prices: Avg {sum(prices)/len(prices):,.1f} and highest {max(prices):,}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="blueviolet", bins=range(0, 1000, 10))
plt.show()
```

---

## Let’s count items per category  
**Note:** The Python method `Counter()` is super useful to count things and classify them.

```python
from collections import Counter

category_counts = Counter() # A special dictionary designed for counting things. It starts empty.
for item in items:
    category_counts[item.category] += 1

categories = category_counts.keys()  # This creates a list of all the categories that were found.
counts = [category_counts[category] for category in categories]  # List of counts for each category

# Bar chart by category
plt.figure(figsize=(15, 6))
plt.bar(categories, counts, color="goldenrod")
plt.title('How many in each category')
plt.xlabel('Categories')
plt.ylabel('Count')

plt.xticks(rotation=30, ha='right')

# Add value labels on top of each bar
for i, v in enumerate(counts):
    plt.text(i, v, f"{v:,}", ha='center', va='bottom')

# Display the chart
plt.show()
```
![categories](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img07_items_per_categories.png)
---

## Objective  
When we analyze the training data, we are looking for a balanced dataset. If there is a subset that prevails, we risk our Model to be too specific data driven in that group and have lower performance in other subsets. This could lead into **“Memory Issues”** where model start “forgetting” and performance decrease in task related to other categories. 

A balanced dataset ensures that the model generalizes well across all categories. If one subset of data is significantly more frequent, the model may overfit to it, learning patterns that do not apply to less common subsets. This imbalance can cause the model to struggle with rare cases, leading to poor performance in real-world applications. The term "memory issues" refers to a phenomenon where the model prioritizes frequently seen data and gradually performs worse on less frequent categories, resembling a form of **"catastrophic forgetting"** seen in machine learning.



In our scenario, Automotive is the most dominant categories. Let’s craft a dataset which is more balanced in terms of prices. Less heavily scewed to cheap items, with an average that's higher than $60. Try to balance out the categories - fewer Automotive items.

---

## Creating a Balanced Dataset  

```python
from collections import defaultdict
import numpy as np
import random

# Create a dictionary with a key for each price from $1 to $999 
# Store items in lists based on their rounded price

slots = defaultdict(list)  # A special dictionary that provides default values for non-existing keys
for item in items:
    slots[round(item.price)].append(item)
```

---

```python
# Create a dataset called "sample" which takes items more evenly across price ranges
# Prioritizes non-Automotive items
# Set random seed for reproducibility

np.random.seed(42)
random.seed(42)

sample = []
for i in range(1, 1000):
    slot = slots[i]  # Get all items with a rounded price of 'i'
    
    # If the price is 240 or higher, include all items
    if i >= 240:
        sample.extend(slot)
    
    # If there are 1200 or fewer items at this price, include all
    elif len(slot) <= 1200:
        sample.extend(slot)
        
    # If there are more than 1200 items, prioritize non-Automotive items
    else:
        weights = np.array([1 if item.category == 'Automotive' else 5 for item in slot])
        weights = weights / np.sum(weights)  # Normalize weights
        selected_indices = np.random.choice(
            len(slot), size=1200, replace=False, p=weights
        )
        selected = [slot[i] for i in selected_indices]
        sample.extend(selected)

print(f"There are {len(sample):,} items in the sample")
```
**Output:**  
*There are 408,635 items in the sample*

```python
# Plot the distribution of prices in sample

prices = [float(item.price) for item in sample]
plt.figure(figsize=(15, 10))
plt.title(f"Avg {sum(prices)/len(prices):.2f} and highest {max(prices):,.2f}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="darkblue", bins=range(0, 1000, 10))
plt.show()
```
![sample_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img08_sample_dist.png)

```python
# OK, we did well in terms of raising the average price and having a smooth-ish population of prices
# Let's see the categories

category_counts = Counter()
for item in sample:
    category_counts[item.category]+=1

categories = category_counts.keys()
counts = [category_counts[category] for category in categories]

# Create bar chart
plt.figure(figsize=(15, 6))
plt.bar(categories, counts, color="lightgreen")

# Customize the chart
plt.title('How many in each category')
plt.xlabel('Categories')
plt.ylabel('Count')

plt.xticks(rotation=30, ha='right')

# Add value labels on top of each bar
for i, v in enumerate(counts):
    plt.text(i, v, f"{v:,}", ha='center', va='bottom')

# Display the chart
plt.show()
```
![new_sample_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img09_new_sample_dist.png)

```python
# Automotive still in the lead, but improved somewhat
# For another perspective, let's look at a pie

plt.figure(figsize=(12, 10))
plt.pie(counts, labels=categories, autopct='%1.0f%%', startangle=90)

# Add a circle at the center to create a donut chart (optional)
centre_circle = plt.Circle((0,0), 0.70, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)
plt.title('Categories')

# Equal aspect ratio ensures that pie is drawn as a circle
plt.axis('equal')  

plt.show()
```
![pie_chart](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img10_new_sample_dist_pie.png)

# Dataset Curated!

We've crafted an excellent dataset.  
Let's do some final checks  

### Looking some insights as data correclation!!
```python
# How does the price vary with the character count of the prompt?

sizes = [len(item.prompt) for item in sample]
prices = [item.price for item in sample]

# Create the scatter plot
plt.figure(figsize=(15, 8))
plt.scatter(sizes, prices, s=0.2, color="red")

# Add labels and title
plt.xlabel('Size')
plt.ylabel('Price')
plt.title('Is there a simple correlation?')

# Display the plot
plt.show()
```
![price_size](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img11_price_vs_item_descroption_size.png)

There is no correlation between the size of the item description and the price. 

```python
# Let's check the last 10 tokens associated to the text prompt
def report(item):
    prompt = item.prompt
    tokens = Item.tokenizer.encode(item.prompt)
    print(prompt)
    print(tokens[-10:])
    print(Item.tokenizer.batch_decode(tokens[-10:]))
```
```python
report(sample[398000])
```
**Output:**
**How much does this cost to the nearest dollar?

MonoRS Coilovers Lowering Kit Made For Scion FRS Fully Adjustable, Set of 4
MonoRS Coilover damper kit by Godspeed Project are intermediate suspension upgrade setup for daily and Sunday club racing. Lowering your car with improved springs over factory and paired with Mono-tubo shocks with valving that allows 32 levels of rebound adjustment to improve handling without sacrifice comfort. Ride height can easily be adjusted by twisting the lower mount bracket. In order to keep weight gain at the minimum, most of attachments and accessories are CNC machined from billet aluminum. Koyo bearings are used when camber plate top mount is applicable depends on car models. To assure that our customers are getting high quality products, MonoRS coilovers are covered by 12 months limited warranty by the manufacturer from

Price is $765.00  
[279, 14290, 505, 271, 7117, 374, 400, 22240, 13, 410]  
[' the', ' manufacturer', ' from', '\n\n', 'Price', ' is', ' $', '765', '.', '00']**  

## Observation
An interesting thing about the Llama tokenizer is that **every number** from *1 to 999* gets mapped to **1 token**,same for gpt-4o.

This is not true of **qwen2**, **gemma** and **phi3**, which all map *individual digits* to *tokens*. This does turn out to be a bit useful for our project, although it's not an essential requirement.

# Finally

It's time to break down our data into a training, test and validation dataset.

It's typical to use **5%-10%** of your data for **testing purposes**, but 

We actually we have far more than we need at this point. We'll take **400,000** points for training, and we'll reserve **2,000** for **testing**, although we won't use all of them.

We must shuffle the samples and separate the datapoint in training and validation(test)
```python
random.seed(42)
random.shuffle(sample)
train = sample[:400_000]
test = sample[400_000:402_000]
print(f"Divided into a training set of {len(train):,} items and test set of {len(test):,} items")
```

```python
print(train[0].prompt)
```
**Output:**
**How much does this cost to the nearest dollar?

Delphi FG0166 Fuel Pump Module
Delphi brings 80 years of OE Heritage into each Delphi pump, ensuring quality and fitment for each Delphi part. Part is validated, tested and matched to the right vehicle application Delphi brings 80 years of OE Heritage into each Delphi assembly, ensuring quality and fitment for each Delphi part Always be sure to check and clean fuel tank to avoid unnecessary returns Rigorous OE-testing ensures the pump can withstand extreme temperatures Brand Delphi, Fit Type Vehicle Specific Fit, Dimensions LxWxH 19.7 x 7.7 x 5.1 inches, Weight 2.2 Pounds, Auto Part Position Unknown, Operation Mode Mechanical, Manufacturer Delphi, Model FUEL PUMP, Dimensions 19.7

Price is $227.00**

```python
print(test[0].test_prompt())
```
**Output:**
**How much does this cost to the nearest dollar?

OEM AC Compressor w/A/C Repair Kit For Ford F150 F-150 V8 & Lincoln Mark LT 2007 2008 - BuyAutoParts NEW
As one of the world's largest automotive parts suppliers, our parts are trusted every day by mechanics and vehicle owners worldwide. This A/C Compressor and Components Kit is manufactured and tested to the strictest OE standards for unparalleled performance. Built for trouble-free ownership and 100% visually inspected and quality tested, this A/C Compressor and Components Kit is backed by our 100% satisfaction guarantee. Guaranteed Exact Fit for easy installation 100% BRAND NEW, premium ISO/TS 16949 quality - tested to meet or exceed OEM specifications Engineered for superior durability, backed by industry-leading unlimited-mileage warranty Included in this K

Price is $ **

# Showing snapshot of the price distribution of the sample

```python
# Plot the distribution of prices in the first 250 test points

prices = [float(item.price) for item in test[:250]]
plt.figure(figsize=(15, 6))
plt.title(f"Avg {sum(prices)/len(prices):.2f} and highest {max(prices):,.2f}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="darkblue", bins=range(0, 1000, 10))
plt.show()
```
![snap](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img12_snapshot_sample_250_items_training.png)

# Finally - upload your brand new dataset

Convert to prompts and upload to HuggingFace hub
```python
train_prompts = [item.prompt for item in train]
train_prices = [item.price for item in train]
test_prompts = [item.test_prompt() for item in test]
test_prices = [item.price for item in test]
```

```python
# Create a Dataset from the lists

train_dataset = Dataset.from_dict({"text": train_prompts, "price": train_prices})
test_dataset = Dataset.from_dict({"text": test_prompts, "price": test_prices})
dataset = DatasetDict({
    "train": train_dataset,
    "test": test_dataset
})
```

```python
# Uncomment these lines if you're ready to push to the hub, and replace my name with your HF username

HF_USER = "USERNAME"
DATASET_NAME = f"{HF_USER}/pricer-data"
dataset.push_to_hub(DATASET_NAME, private=True)
```

```python
# One more thing!
# Let's pickle the training and test dataset so we don't have to execute all this code next time!

with open('train.pkl', 'wb') as file:
    pickle.dump(train, file)
with open('test.pkl', 'wb') as file:
    pickle.dump(test, file)
```
---
# Training the Model 
In this section we explored different traditional ML model vs LLM model for our use case scenario. I skipped the details of traditional ML. 

## Importance of starting with a Baseline Model
- Start cheap and simple  
- An LLM might not be the right solution  (generating a number predicted is not a natural function of LLM) 

### Traditional ML model as starting Point:

* Feature engineering & Linear Regression
* Bag of words & Linear Regression
* word2vec & Random Forest
* word2vec & SVR (Support Vector Regression)

![traditional_ML](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img13_traditional_ML.png)

---
## 1. Testing Frontier Models:

Now we put Frontier Models to the test.

### 2 important points:

It's important to appreciate that we aren't Training the frontier models. We're only providing them with the Test dataset to see how they perform. They don't gain the benefit of the 400,000 training examples that we provided to the Traditional ML models.

HAVING SAID THAT...

It's entirely possible that in their monstrously large training data, they've already been exposed to all the products in the training AND the test set. So there could be test "contamination" here which gives them an unfair advantage. We should keep that in mind.

> :memo: **Note:**
>
> To test the model You write a function of this form:
>
> ```python
> def my_prediction_function(item):
>     # my code here
>     return my_estimate
> ```
>
> And then you pass it Tester to evaluate the model
>
> ```python 
> Tester.test(my_prediction_function)
> ```

Check the  Tester code [here](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/code_examples/testing.py)

<details><summary><strong> **Tester Code Explanation** </strong></summary>

# Tester code 
This code is a testing framework that evaluates price prediction models. Let me break it down step by step:

## The `Tester` Class

This class helps you test how well a function predicts prices by comparing predictions against actual prices.

### Initialization (Setup)
```python
def __init__(self, predictor, title=None, data=test, size=250):
```
When you create a new Tester:
- `predictor`: The function that predicts prices
- `title`: Optional name for your test
- `data`: The dataset to test with (defaults to a variable called `test`)
- `size`: How many data points to test (defaults to 250)

It also creates empty lists to store results:
- `guesses`: Predicted prices
- `truths`: Actual prices
- `errors`: How far off each prediction was
- `sles`: Square Log Errors (a special error measurement)
- `colors`: Color codes for visualizing results

### Color Coding Results
```python
def color_for(self, error, truth):
```
This method assigns a color based on how accurate the prediction was:
- `green`: Very accurate (error < 40 or less than 20% off)
- `orange`: Somewhat accurate (error < 80 or less than 40% off)
- `red`: Inaccurate (everything else)

### Testing a Single Data Point
```python
def run_datapoint(self, i):
```
For each item in your data:
1. Gets a prediction from your function
2. Finds the actual price (`truth`)
3. Calculates how far off you were (`error`)
4. Calculates a logarithmic error (`sle`)
5. Assigns a color based on accuracy
6. Saves all results to the lists
7. Prints a color-coded line showing the results

### Visualizing Results
```python
def chart(self, title):
```
Creates a scatter plot where:
- X-axis shows actual prices
- Y-axis shows predicted prices
- Perfect predictions would fall on a blue diagonal line
- Each prediction is a dot colored by its accuracy

### Generating a Summary Report
```python
def report(self):
```
1. Calculates average error across all predictions
2. Calculates Root Mean Square Logarithmic Error (RMSLE) - a common metric
3. Counts how many "green" (accurate) predictions you had
4. Creates a title with summary statistics
5. Displays the chart with this title

### Running the Full Test
```python
def run(self):
```
1. Tests each data point one by one
2. Generates the final report

### Class Method for Quick Testing
```python
@classmethod
def test(cls, function):
```
A shortcut to create a Tester and run it in one line: `Tester.test(your_function)`

## What This Code Is For

This code helps evaluate how good a price prediction model is by:
1. Testing it on many examples
2. Showing color-coded results for each prediction
3. Visualizing all predictions in a chart
4. Calculating overall performance metrics
</details>

* Check Week 6 Day 3 lab for reviewing the ML examples 

```python
# imports

import os
import re
import math
import json
import random
from dotenv import load_dotenv
from huggingface_hub import login
from items import Item
import matplotlib.pyplot as plt
import numpy as np
import pickle
from collections import Counter
from openai import OpenAI
from anthropic import Anthropic

# moved our Tester into a separate package
# call it with Tester.test(function_name, test_dataset)

from testing import Tester
```
```python
# environment

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')
os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')
# Log in to HuggingFace

hf_token = os.environ['HF_TOKEN']
login(hf_token, add_to_git_credential=True)

openai = OpenAI()
claude = Anthropic()
%matplotlib inline
```
### Loading the pkl files
Let's avoid curating all our data again! Load in the pickle files

If you didn't already create these, you can also download them from my google drive
https://drive.google.com/drive/folders/1f_IZGybvs9o0J5sb3xmtTEQB3BXllzrW

But note that the files are quite large - you might need to get a coffee!

```python
with open('train.pkl', 'rb') as file:
    train = pickle.load(file)

with open('test.pkl', 'rb') as file:
    test = pickle.load(file)
```
### First, the humble but mighty GPT-4o-mini
```python
# First let's work on a good prompt for a Frontier model
# Notice that I'm removing the " to the nearest dollar"
# When we train our own models, we'll need to make the problem as easy as possible, 
# but a Frontier model needs no such simplification.

def messages_for(item):
    system_message = "You estimate prices of items. Reply only with the price, no explanation"
    user_prompt = item.test_prompt().replace(" to the nearest dollar","").replace("\n\nPrice is $","")
    return [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_prompt},
        {"role": "assistant", "content": "Price is $"}
    ]
```
```python
# Try this out
messages_for(test[0])
```
>[Note:]
> ### A utility function to extract the price from a string
>```python
> def get_price(s):
>     s = s.replace('$','').replace(',','')
>    match = re.search(r"[-+]?\d*\.\d+|\d+", s)
>    return float(match.group()) if match else 0
>```
> ```python
> get_price("The price is roughly $99.99 because blah blah")
> ```
> `99.99`

### Testing OpenAi gpt-4o-mini
```python
# The function for gpt-4o-mini

def gpt_4o_mini(item):
    response = openai.chat.completions.create(
        model="gpt-4o-mini", 
        messages=messages_for(item),
        seed=42,
        max_tokens=5 #limiting the #of tokens in the output safeguard $$
    )
    reply = response.choices[0].message.content
    return get_price(reply)
```
```python
#passing the function and the test data (trying the best without been trained)
#this cost some $ cents 
Tester.test(gpt_4o_mini, test)
```
![gpt_4o_mini_no_training](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img14_testing_gpt4o_no_training.png)

### Testing Claude 3.5 Sonet 

```python
def claude_3_point_5_sonnet(item):
    messages = messages_for(item)
    system_message = messages[0]['content'] # here we are stracting the system messages from the dic of items
    messages = messages[1:] # here we are passing the rest of items without the systems message
    response = claude.messages.create(
        model="claude-3-5-sonnet-20240620",
        max_tokens=5,
        system=system_message,
        messages=messages
    )
    reply = response.content[0].text
    return get_price(reply)
```
```python
# The function for Claude 3.5 Sonnet
# It also cost me about 1-2 cents to run this (pricing may vary by region)
Tester.test(claude_3_point_5_sonnet, test)
```
![claude_no_training](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img15_testing_claude_3_5_sonet_no_training.png)

---
## 2. Training/ Fine-tunning Frontier Models:

OpenAI recommends fine-tuning with populations of *50-100* examples
But as our examples are very small, I'm suggesting we go with 200 examples (and 1 *epoch*)
```python
fine_tune_train = train[:200]
fine_tune_validation = train[200:250]
```
> :memo: **Note:Epoch **
> <details><summary><strong> Epoch meaning </strong></summary>
> In the context of machine learning and training models, an "epoch" refers to one complete pass through the entire training dataset. During this pass, the model learns from the data and updates its weights based on the error it encounters.
>
> Here’s a breakdown of the concept:
> 	1. Complete Data Pass: An epoch represents how many times the learning algorithm has seen the entire training dataset.
> 	2. Weight Updates: After each epoch, the model typically adjusts its internal parameters (weights) based on how well it performed on the data during that epoch.
>  	3. Iterations and Batches: Often, data is processed in smaller batches rather than all at once. Even if processing in batches (mini-batch gradient descent), an epoch is still counted as one complete pass through the entire dataset.
>  	4. Multiple Epochs: Training usually involves multiple epochs, allowing the model to gradually converge towards an optimal set of weights.
>  	5. Performance Monitoring: At the end of each epoch, model performance is often evaluated (e.g., on a validation set) to check for improvements or to monitor for overfitting.
In summary, epochs are crucial to the model training process, as they dictate how many times the algorithm learns from the data
></details>

### Step 1
Prepare our data for fine-tuning in JSONL (JSON Lines) format and upload to OpenAI

```python
# First let's work on a good prompt for a Frontier model
# Notice that I'm removing the " to the nearest dollar"
# When we train our own models, we'll need to make the problem as easy as possible, 
# but a Frontier model needs no such simplification.

def messages_for(item):
    system_message = "You estimate prices of items. Reply only with the price, no explanation"
    user_prompt = item.test_prompt().replace(" to the nearest dollar","").replace("\n\nPrice is $","")
    return [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_prompt},
        {"role": "assistant", "content": f"Price is ${item.price:.2f}"}
    ]
```
```python
#you can test this prompt with
messages_for(train[0])
```

Convert the items into a list of json objects - a "jsonl" string
```python
# Each row represents a message in the form:
# {"messages" : [{"role": "system", "content": "You estimate prices...


def make_jsonl(items):
    result = ""
    for item in items:
        messages = messages_for(item)
        messages_str = json.dumps(messages) # we convert the messages to string
        result += '{"messages": ' + messages_str +'}\n' #we put it in jsonl format
    return result.strip() #removing any space or newlines cleaner output
```

Convert the items into jsonl and write them to a file
```python
def write_jsonl(items, filename):
    with open(filename, "w") as f:
        jsonl = make_jsonl(items)
        f.write(jsonl)
```
we save the train and validation data into two separate files to be oploaded to OpenAi
```python
write_jsonl(fine_tune_train, "fine_tune_train.jsonl")

write_jsonl(fine_tune_validation, "fine_tune_validation.jsonl")
```
This is how the file looks like

![training_file_jsonl_format](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img16_training_file_jsonl.png)

### step 2: Loading the training and validation file to OpenAi

```python
#THIS IS HOW WE UPLOAD THE DATASET TO OPENAI
with open("fine_tune_train.jsonl", "rb") as f: #notice "rb" ->>read binary we pass the data in binary 
    train_file = openai.files.create(file=f, purpose="fine-tune") 

with open("fine_tune_validation.jsonl", "rb") as f:
    validation_file = openai.files.create(file=f, purpose="fine-tune")
```

```python
#to check the loading status
train_file
```
Notice the Status **"processed"**

`FileObject(id='file-1Mzc4RS6Tn9SdDtLqMncjm', bytes=188742, created_at=1743079012, filename='fine_tune_train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None)`

```python
validation_file
```
`FileObject(id='file-9ThDorj9CEn9YRSRd6y4eq', bytes=47085, created_at=1743079051, filename='fine_tune_validation.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None)
`
### Step 3: Monitoring Training runs 

Weights and Biases -  free platform for monitoring training runs.
Weights and Biases is integrated with OpenAI for fine-tuning.

First set up your weights & biases free account at:

https://wandb.ai

From the Avatar >> Settings menu, near the bottom, you can create an API key.

Then visit the OpenAI dashboard at:

https://platform.openai.com/account/organization

In the integrations section, you can add your Weights & Biases API key.

Once done:

We give a name to this new trained Model: {"project": "gpt-pricer"}
```python
wandb_integration = {"type": "wandb", "wandb": {"project": "gpt-pricer"}}
```
We can retreive the training file id by 
```python
train_file.id
```
`'file-1Mzc4RS6Tn9SdDtLqMncjm'`

Now let's fine-tune the model !!

> :warning: **Warning:** This step cost $ money. It depends on the amount of data we are using. 

```python
openai.fine_tuning.jobs.create(
    training_file=train_file.id, #this is the id of the file sent to OpenAi
    validation_file=validation_file.id, #same for the validation file
    model="gpt-4o-mini-2024-07-18",
    seed=42,
    hyperparameters={"n_epochs": 1}, # optional #"epoch" refers to one complete pass through the entire training dataset
    integrations = [wandb_integration], #optional 
    suffix="pricer" #optional
)
```
`
FineTuningJob(id='ftjob-SzYMjZoOvgQjuDjlTcNZdmEU', created_at=1743085728, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=1), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-XSnZxwnBHejH7h1JSCKFejEl', result_files=[], seed=42, status='validating_files', trained_tokens=None, training_file='file-1Mzc4RS6Tn9SdDtLqMncjm', validation_file='file-9ThDorj9CEn9YRSRd6y4eq', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='gpt-pricer', entity=None, name=None, tags=None, run_id='ftjob-SzYMjZoOvgQjuDjlTcNZdmEU'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs=1)), type='supervised'), user_provided_suffix='pricer', metadata=None)
`
Once it's done, we received an email from OpenAi
![openai_finetune_confirmation]()


