# Training a Model  

## Requires Data  
Here are some available resources where to get datasets:  
- Your own proprietary data  
- [Kaggle](https://www.kaggle.com/)  
- [HuggingFace datasets](https://huggingface.co/datasets)  
- **Synthetic data** (fictitious data generated by a model) – recommended in some cases  
- Specific companies do it → [scale.com](https://scale.com/)  

## Data Stages (as seen in the course on LLM engineering)  
1. **Investigate** – Understanding how well populated the dataset is and its quality  
2. **Parse** – Formatting the data for easy processing  
3. **Visualize** – Examining distributions using graphs or visual tools  
4. **Assess Data Quality**  
5. **Curate** – Crafting the dataset, filtering out unnecessary elements, and addressing imbalance  
6. **Save**  

---

## Use Case: Training a Model to Predict Item Prices Based on Descriptions  

We use a dataset from HuggingFace related to Amazon product reviews:  
🔗 [Amazon Reviews 2023 Dataset](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023)  
🔗 [Folder with all product datasets](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/tree/main/raw/meta_categories)  

---

## **1. Investigate**  

We started by retrieving the dataset:  

```python
from huggingface_hub import login
from datasets import load_dataset, Dataset, DatasetDict
import os

# Log in to HuggingFace
hf_token = os.environ['HF_TOKEN']
login(hf_token, add_to_git_credential=True)

# Load in our dataset
dataset = load_dataset("McAuley-Lab/Amazon-Reviews-2023", "raw_meta_Appliances", split="full", trust_remote_code=True)

# Checking how many items there are
print(f"Number of Appliances: {len(dataset):,}")
```

**Output:**  
```
Number of Appliances: 94,327
```

---

## **2. Parsing the Data**  

Investigate a particular data point:  

```python
datapoint = dataset[0]
print(datapoint["title"])
print(datapoint["description"])
print(datapoint["features"])
print(datapoint["details"])
print(datapoint["price"])
```

We noticed that some products **do not have a price value**. This is an issue since we need the price for training. We need to filter out items without prices.  

**How many have prices?**  

```python
prices = 0
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            prices += 1
    except ValueError:
        pass

print(f"There are {prices:,} items with prices, which is {prices/len(dataset)*100:,.1f}%")
```

**Output:**  
```
There are 46,726 items with prices, which is 49.5%
```

### Checking the Length of Each Item Description  

```python
prices = []
lengths = []
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            prices.append(price)
            contents = datapoint["title"] + str(datapoint["description"]) + str(datapoint["features"]) + str(datapoint["details"])
            lengths.append(len(contents))
    except ValueError:
        pass
```

---

## **3. Visualizing the Data**  

### Distribution of Item Content Lengths  

We use Matplotlib for visualization:  

```python
import matplotlib.pyplot as plt
%matplotlib inline

# Plot the distribution of item content lengths
plt.figure(figsize=(15, 6))
plt.title(f"Lengths: Avg {sum(lengths)/len(lengths):,.0f} and highest {max(lengths):,}\n")
plt.xlabel('Length (chars)')
plt.ylabel('Count')
plt.hist(lengths, rwidth=0.7, color="lightblue", bins=range(0, 6000, 100))
plt.show()
```
![item_length](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img01_item_detail_lenght.png)

### Distribution of Prices  

```python
# Plot the distribution of prices
plt.figure(figsize=(15, 6))
plt.title(f"Prices: Avg {sum(prices)/len(prices):,.2f} and highest {max(prices):,}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="orange", bins=range(0, 1000, 10))
plt.show()
```
![price_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img02_item_ave_price.png)

**Observations:**  
- Some items have **very high prices**, which could impact the results.  

---

## **4. Curating the Data**  

## Data Curation Part 1
### Considerations:  
- **Cost & Memory Factor:**  
  - The number of tokens affects training costs for Closed-Source models.  
  - **More tokens → More memory required → Harder to train.**  

Since we're working with **Closed-Source models**, we must be mindful of **token usage**.  
- We will **limit the number of tokens** used to train the model.  
- We need to **curate** each item to fit within a certain token limit.  

### **Curating Strategy:**  
- Select items **priced between $1 and $999**  
- Create **Item instances** that:  
  - **Truncate text** to fit within **180 tokens** using the right tokenizer  
  - **Generate a prompt** for training  

**Tokenizer Used:** Meta-Llama-3.1-8B  
- If an item doesn’t have sufficient characters, it is **rejected**  

![item_class](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img03_item_class.png)
### **Why 180 Tokens?**  
This is a **hyperparameter** – determined through **trial and error**.  
- Needs to be **large enough** to capture useful price-related information.  
- Needs to be **small enough** for efficient training.  
- Mimics **inference-time inputs** (short descriptions of 1-2 sentences).  

💡 *If you experiment with different token limits, you might get better results!*  

---

###  **4.1. A Potential Issue in the `Item` Code**  

<details>
<summary><strong>Issue: Truncated Words and Sentences</strong></summary>

### Problem Breakdown:  
1. **Chopped Sentences & Words**  
   - The code truncates text based on character count (`contents = contents[:CEILING_CHARS]`).  
   - Then tokenizes the text.  
   - Then further trims based on token count (`tokens = tokens[:MAX_TOKENS]`).  
   - Finally, it decodes back into text.  

2. **Misleading Content for the Model**  
   - Truncation can **cut sentences mid-way**, leading to **loss of meaning**.  
   - Example:  
     ```
     "This product is highly durable and..."
     ```
     could become  
     ```
     "This product is highly dur..."
     ```
   - **Tokenization can lose meaning** if a word is cut off (e.g., `"application"` → `"applicat"`).  

3. **Impact on Model Training**  
   - **Reduced Accuracy**: Model learns from incomplete/misleading data.  
   - **Bias**: Some product descriptions might be disproportionately affected.  
   - **Instability**: Model might struggle to converge.  

### **Possible Solutions:**  
- **Sentence-Based Truncation**: Cut at sentence boundaries.  
- **Word-Based Truncation Before Tokenization**: Avoid splitting words.  
- **Summarization**: Use NLP techniques to shorten descriptions meaningfully.  
- **Increase `MAX_TOKEN` and `CEILING_CHARS`**: If feasible, raise limits to reduce unwanted truncation.  

---

**Note from Ed:**  
> "Hey Luis - `CEILING_CHARS` was meant to improve processing efficiency, not to truncate important data. If it's cutting meaningful content, that's a bug! A quick fix: triple `CEILING_CHARS`. The model should handle tokenized fragments, but real truncation affecting `MAX_TOKENS` is a problem. Let me know!"  

</details>

## Creating Item Objects for Each Data Point with a Price

```python
# Create an Item object for each with a price
items = []
for datapoint in dataset:
    try:
        price = float(datapoint["price"])
        if price > 0:
            item = Item(datapoint, price) # creating an Item 
            if item.include:
                items.append(item)
    except ValueError as e:
        pass

print(f"There are {len(items):,} items")
```

**Output:**
```
There are 29,191 items
```

### Looking at the First Item
```python
# Look at the first item
items[0]
```
**Output:**
```
<WD12X10327 Rack Roller and stud assembly Kit (4 Pack) by AMI PARTS Replaces AP4980629 PS3486910 1811003 = $8.99>
```

### Investigating the Prompt Used During Training

```python
# Investigate the prompt that will be used during training - the model learns to complete this
print(items[1].prompt)
```
**Output:**
```
How much does this cost to the nearest dollar?

Door Pivot Block - Compatible Kenmore KitchenAid Maytag Whirlpool Refrigerator - Replaces - Quick DIY Repair Solution
Pivot Block For Vernicle Mullion Strip On Door - A high-quality exact equivalent for part numbers and Compatibility with major brands - Door Guide is compatible with Whirlpool, Amana, Dacor, Gaggenau, Hardwick, Jenn-Air, Kenmore, KitchenAid, and Maytag. Quick DIY repair - Refrigerator Door Guide Pivot Block Replacement will help if your appliance door doesn't open or close. Wear work gloves to protect your hands during the repair process. Attentive support - If you are uncertain about whether the block fits your refrigerator, we will help. We generally put forth a valiant effort to guarantee you are totally

Price is $17.00
```

### Investigating the Prompt Used During Testing

```python
# Investigate the prompt that will be used during testing - the model has to complete this
print(items[0].test_prompt())
```
**Output:**
```
How much does this cost to the nearest dollar?

Rack Roller and stud assembly Kit (4 Pack) by AMI PARTS Replaces
PARTS NUMBER The dishwasher top rack wheels and stud assembly Kit （4 pcs） SCOPE OF APPLICATION The dishwasher works with most top name brands,If you are not sure if part is correct, ask us in Customer questions & answers section or visiting the AMI PARTS storefront.We’re happy to help ensure you select the correct part for your Rack Roller and stud REPLACES PART FIXES SYMPTOMS Door won’t close | Not cleaning dishes properly | Noisy | Door latch failure QUALITY WARRANTY The replacement part is made from durable high quality material and well-tested by manufacturer.For any reason you’re not satisfied,you can ask for a replacement or full refund Brand Name AMI PARTS, Model

Price is $
```

## Visualizing Token Distribution After Filtering

```python
# Plot the distribution of token counts
tokens = [item.token_count for item in items]
plt.figure(figsize=(15, 6))
plt.title(f"Token counts: Avg {sum(tokens)/len(tokens):,.1f} and highest {max(tokens):,}\n")
plt.xlabel('Length (tokens)')
plt.ylabel('Count')
plt.hist(tokens, rwidth=0.7, color="green", bins=range(0, 300, 10))
plt.show()
```
![token_length_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img04_token_length_dist.png)

## Analyzing the New Price Distribution

```python
# Plot the distribution of prices
prices = [item.price for item in items]
plt.figure(figsize=(15, 6))
plt.title(f"Prices: Avg {sum(prices)/len(prices):,.1f} and highest {max(prices):,}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="purple", bins=range(0, 1000, 10))
plt.show()
```
![new_price_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img05_new_item_ave_price_after_180_Token.png)

## Sidenote
If you like the variety of colors that matplotlib can use in its charts, you should bookmark this:
https://matplotlib.org/stable/gallery/color/named_colors.html

## Data Curation Part 2

Today we'll extend our dataset to a greater coverage, and craft it into an excellent dataset for training.  
Data curation can seem less exciting than other things we work on, but it's a crucial part of the LLM engineers' responsibility and an important craft to hone, so that you can build your own commercial solutions with high quality datasets.

<detail><summary><strong>Important Note - read me first please </strong></summary>

We are about to craft a massive dataset of 400,000 items covering multiple types of product. In Week 7 we will be using this data to train our own model. It's a pretty big dataset, and depending on the GPU you select, training could take 20+ hours. It will be really good fun, but it could cost a few dollars in compute units.

As an alternative, if you want to keep things quick & low cost, you can work with a smaller dataset focused only on Home Appliances. You'll be able to cover the same learning points; the results will be good -- not quite as good as the full dataset, but still pretty amazing! If you'd prefer to do this, I've set up an alternative jupyter notebook in this folder called `lite.ipynb` that you should use in place of this one.

Also, if you'd prefer, you can shortcut running all this data curation by downloading the pickle files that we save in the last cell. The pickle files are available here: https://drive.google.com/drive/folders/1f_IZGybvs9o0J5sb3xmtTEQB3BXllzrW
</detail>

```python
# imports
import os
import random
from dotenv import load_dotenv
from huggingface_hub import login
from datasets import load_dataset, Dataset, DatasetDict
from items import Item
from loaders import ItemLoader
import matplotlib.pyplot as plt
from collections import Counter, defaultdict
import numpy as np
import pickle

# environment

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')
os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')

# Log in to HuggingFace

hf_token = os.environ['HF_TOKEN']
login(hf_token, add_to_git_credential=True)

%matplotlib inline
```
## The ItemLoader code
Look in loaders.py - there's some useful code to make life easier for us

Here is your content formatted in **Markdown**:

```markdown
```python
# Load one dataset at a time -> In this case only the Appliances
items = ItemLoader("Appliances").load()
```

## Now to SCALE UP
Let's look at all datasets of all the items that you might find in a large home retail store - electrical, electronic, office and related, but not clothes / beauty / books.

```python
dataset_names = [
    "Automotive",
    "Electronics",
    "Office_Products",
    "Tools_and_Home_Improvement",
    "Cell_Phones_and_Accessories",
    "Toys_and_Games",
    "Appliances",
    "Musical_Instruments",
]
```

# Now, time for a coffee break!!  
### By the way, I put the biggest datasets first.. it gets faster.

```python
items = []
for dataset_name in dataset_names:
    loader = ItemLoader(dataset_name)
    items.extend(loader.load(workers=10))
```

```python
print(f"A grand total of {len(items):,} items")
```

**Output:**  
*A grand total of 2,811,408 items*

---

[Note]: Recommendation of HW setting for handling heavy tasks like this.  
In most cases, for an **LLM engineer**, a machine with **50GB drive space and 16GB memory** is sufficient; you can survive on less, and more is nice but a luxury.  

Personally, I'm an advocate for using **cloud servers** for GPUs. Some people like to invest in hefty GPU boxes for local training, and I get that—it's satisfying to run everything locally. But you could end up dropping **$4,000** for a high-end GPU box, and in **12 months** it might be obsolete. 
I personally prefer to run on **the cloud**, and for **under $20 a month**, I get access to a lot of power.  

---

## Plot the distribution of prices
```python
prices = [item.price for item in items]
plt.figure(figsize=(15, 6))
plt.title(f"Prices: Avg {sum(prices)/len(prices):,.1f} and highest {max(prices):,}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="blueviolet", bins=range(0, 1000, 10))
plt.show()
```

---

## Let’s count items per category  
**Note:** The Python method `Counter()` is super useful to count things and classify them.

```python
from collections import Counter

category_counts = Counter() # A special dictionary designed for counting things. It starts empty.
for item in items:
    category_counts[item.category] += 1

categories = category_counts.keys()  # This creates a list of all the categories that were found.
counts = [category_counts[category] for category in categories]  # List of counts for each category

# Bar chart by category
plt.figure(figsize=(15, 6))
plt.bar(categories, counts, color="goldenrod")
plt.title('How many in each category')
plt.xlabel('Categories')
plt.ylabel('Count')

plt.xticks(rotation=30, ha='right')

# Add value labels on top of each bar
for i, v in enumerate(counts):
    plt.text(i, v, f"{v:,}", ha='center', va='bottom')

# Display the chart
plt.show()
```
![categories](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img07_items_per_categories.png)
---

## Objective  
When we analyze the training data, we are looking for a balanced dataset. If there is a subset that prevails, we risk our Model to be too specific data driven in that group and have lower performance in other subsets. This could lead into **“Memory Issues”** where model start “forgetting” and performance decrease in task related to other categories. 

A balanced dataset ensures that the model generalizes well across all categories. If one subset of data is significantly more frequent, the model may overfit to it, learning patterns that do not apply to less common subsets. This imbalance can cause the model to struggle with rare cases, leading to poor performance in real-world applications. The term "memory issues" refers to a phenomenon where the model prioritizes frequently seen data and gradually performs worse on less frequent categories, resembling a form of **"catastrophic forgetting"** seen in machine learning.



In our scenario, Automotive is the most dominant categories. Let’s craft a dataset which is more balanced in terms of prices. Less heavily scewed to cheap items, with an average that's higher than $60. Try to balance out the categories - fewer Automotive items.

---

## Creating a Balanced Dataset  

```python
from collections import defaultdict
import numpy as np
import random

# Create a dictionary with a key for each price from $1 to $999 
# Store items in lists based on their rounded price

slots = defaultdict(list)  # A special dictionary that provides default values for non-existing keys
for item in items:
    slots[round(item.price)].append(item)
```

---

```python
# Create a dataset called "sample" which takes items more evenly across price ranges
# Prioritizes non-Automotive items
# Set random seed for reproducibility

np.random.seed(42)
random.seed(42)

sample = []
for i in range(1, 1000):
    slot = slots[i]  # Get all items with a rounded price of 'i'
    
    # If the price is 240 or higher, include all items
    if i >= 240:
        sample.extend(slot)
    
    # If there are 1200 or fewer items at this price, include all
    elif len(slot) <= 1200:
        sample.extend(slot)
        
    # If there are more than 1200 items, prioritize non-Automotive items
    else:
        weights = np.array([1 if item.category == 'Automotive' else 5 for item in slot])
        weights = weights / np.sum(weights)  # Normalize weights
        selected_indices = np.random.choice(
            len(slot), size=1200, replace=False, p=weights
        )
        selected = [slot[i] for i in selected_indices]
        sample.extend(selected)

print(f"There are {len(sample):,} items in the sample")
```
**Output:**  
*There are 408,635 items in the sample*

```python
# Plot the distribution of prices in sample

prices = [float(item.price) for item in sample]
plt.figure(figsize=(15, 10))
plt.title(f"Avg {sum(prices)/len(prices):.2f} and highest {max(prices):,.2f}\n")
plt.xlabel('Price ($)')
plt.ylabel('Count')
plt.hist(prices, rwidth=0.7, color="darkblue", bins=range(0, 1000, 10))
plt.show()
```
![sample_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img08_sample_dist.png)

```python
# OK, we did well in terms of raising the average price and having a smooth-ish population of prices
# Let's see the categories

category_counts = Counter()
for item in sample:
    category_counts[item.category]+=1

categories = category_counts.keys()
counts = [category_counts[category] for category in categories]

# Create bar chart
plt.figure(figsize=(15, 6))
plt.bar(categories, counts, color="lightgreen")

# Customize the chart
plt.title('How many in each category')
plt.xlabel('Categories')
plt.ylabel('Count')

plt.xticks(rotation=30, ha='right')

# Add value labels on top of each bar
for i, v in enumerate(counts):
    plt.text(i, v, f"{v:,}", ha='center', va='bottom')

# Display the chart
plt.show()
```
![new_sample_dist](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img09_new_sample_dist.png)

```python
# Automotive still in the lead, but improved somewhat
# For another perspective, let's look at a pie

plt.figure(figsize=(12, 10))
plt.pie(counts, labels=categories, autopct='%1.0f%%', startangle=90)

# Add a circle at the center to create a donut chart (optional)
centre_circle = plt.Circle((0,0), 0.70, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)
plt.title('Categories')

# Equal aspect ratio ensures that pie is drawn as a circle
plt.axis('equal')  

plt.show()
```
![pie_chart](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img10_new_sample_dist_pie.png)

# Dataset Curated!

We've crafted an excellent dataset.  
Let's do some final checks  

### Looking some insights as data correclation!!
```python
# How does the price vary with the character count of the prompt?

sizes = [len(item.prompt) for item in sample]
prices = [item.price for item in sample]

# Create the scatter plot
plt.figure(figsize=(15, 8))
plt.scatter(sizes, prices, s=0.2, color="red")

# Add labels and title
plt.xlabel('Size')
plt.ylabel('Price')
plt.title('Is there a simple correlation?')

# Display the plot
plt.show()
```
![price_size](https://github.com/luismcapriles/llm_engineering_course/blob/main/notes/W6/img11_price_vs_item_descroption_size.png)

There is no correlation between the size of the item description and the price. 

```python
# Let's check the last 10 tokens associated to the text prompt
def report(item):
    prompt = item.prompt
    tokens = Item.tokenizer.encode(item.prompt)
    print(prompt)
    print(tokens[-10:])
    print(Item.tokenizer.batch_decode(tokens[-10:]))
```
```python
report(sample[398000])
```
**Output:**
**How much does this cost to the nearest dollar?

MonoRS Coilovers Lowering Kit Made For Scion FRS Fully Adjustable, Set of 4
MonoRS Coilover damper kit by Godspeed Project are intermediate suspension upgrade setup for daily and Sunday club racing. Lowering your car with improved springs over factory and paired with Mono-tubo shocks with valving that allows 32 levels of rebound adjustment to improve handling without sacrifice comfort. Ride height can easily be adjusted by twisting the lower mount bracket. In order to keep weight gain at the minimum, most of attachments and accessories are CNC machined from billet aluminum. Koyo bearings are used when camber plate top mount is applicable depends on car models. To assure that our customers are getting high quality products, MonoRS coilovers are covered by 12 months limited warranty by the manufacturer from

Price is $765.00
[279, 14290, 505, 271, 7117, 374, 400, 22240, 13, 410]
[' the', ' manufacturer', ' from', '\n\n', 'Price', ' is', ' $', '765', '.', '00']**

## Observation
An interesting thing about the Llama tokenizer is that **every number** from *1 to 999* gets mapped to **1 token**,same for gpt-4o.

This is not true of **qwen2**, **gemma** and **phi3**, which all map *individual digits* to *tokens*. This does turn out to be a bit useful for our project, although it's not an essential requirement.

# Finally

It's time to break down our data into a training, test and validation dataset.

It's typical to use **5%-10%** of your data for **testing purposes**, but 

We actually we have far more than we need at this point. We'll take **400,000** points for training, and we'll reserve **2,000** for **testing**, although we won't use all of them.
