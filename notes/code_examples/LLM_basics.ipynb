{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39d465-1b99-431f-a780-1df2a857ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67677903-b3e3-4f9e-a5f3-929f53dab37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7fa479-9126-4882-a679-db15c27938e1",
   "metadata": {},
   "source": [
    "### LLM interactions:\n",
    "Closed-Source\n",
    "- OpenAi\n",
    "- Anthropic\n",
    "- Gemini\n",
    "\n",
    "  \n",
    "Open-Source\n",
    "- Ollama\n",
    "- Hugging Face\n",
    "- Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e05e86-4ba6-46cb-9f6b-9e77c687b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "import anthropic\n",
    "import google.generativeai\n",
    "from groq import Groq\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cce011-cd68-4422-b3dc-bd9d569962b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants \n",
    "MODEL_GPT = \"gpt-4o-mini\"\n",
    "MODEL_LLAMA= \"llama3.2\"\n",
    "MODEL_ANTHROPIC = \"claude-3-5-haiku-20241022\"\n",
    "MODEL_GEMINI = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550d441-0c31-41b5-ac3c-04c28d3c89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up environment\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "claude_api_key= os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "#Check key works\n",
    "if not openai_api_key:\n",
    "    print (\"No OpenAi API key found\")\n",
    "elif not openai_api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"OpenAi API found but does not start with sk-proj-\")\n",
    "elif openai_api_key.strip() != openai_api_key:\n",
    "    print(\"OpenAI API found but might contain space or tab character - remove them\")\n",
    "else:\n",
    "    print(\"OpenAi API found and looks good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2fe1c-f92a-4c14-bffa-c8a16a712bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create an instance of models\n",
    "openai=OpenAI()\n",
    "groq_model = Groq()\n",
    "claude = anthropic.Anthropic()\n",
    "gemini = genai.Client(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62c870-d25b-478c-9b76-1e0f08480f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#OpenAI structure\n",
    "system_prompt= \"You are a helpful assistant\"\n",
    "user_prompt = \"Explain embedding in simple words\"\n",
    "\n",
    "message =[{\"role\":\"system\",\"content\":system_prompt},{\"role\":\"user\",\"content\":user_prompt}]\n",
    "\n",
    "result = openai.chat.completions.create(model=MODEL_GPT, messages=message)\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc76ac6-a3a7-4c85-aad8-69268c417398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ollama\n",
    "result_ollama = ollama.chat(model=MODEL_LLAMA,messages=message)\n",
    "print(result_ollama[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e68c1e-b736-4d9b-8e72-28543218c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groq Structure uses any model ontop \n",
    "resul_groq = groq_model.chat.completions.create(model=\"llama-3.3-70b-versatile\",messages=message)\n",
    "print(resul_groq.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94a7f4-acaf-42f7-ae48-bcd0a3da3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anthropic Structure \n",
    "#Anthropic needs SYSTEM_message separate from USER_message\n",
    "#also needs Max tokens\n",
    "claude_message = claude.messages.create(\n",
    "    model=MODEL_ANTHROPIC,\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_prompt,\n",
    "    messages=[{\"role\":\"user\",\"content\":user_prompt}]\n",
    ")\n",
    "print(claude_message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31372bc4-39e9-4472-b995-21375c3be6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Gemini Structure\n",
    "gemini_result = gemini.models.generate_content(model= MODEL_GEMINI,contents=user_prompt)\n",
    "print(gemini_result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a182bd6-bd02-4632-aeb5-d3c5ef8aa971",
   "metadata": {},
   "source": [
    "### Streamming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976d9b3-4d93-446f-81cf-679510d14a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenAI Streamming\n",
    "result = openai.chat.completions.create(model=MODEL_GPT, messages=message, stream=True)\n",
    "\n",
    "for chunk in result:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9fc12-ceb4-412a-af54-0dcb3e48eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anthropic Streamming\n",
    "#NOTRE: claude.message.stream() \n",
    "claude_message = claude.messages.stream(\n",
    "    model=MODEL_ANTHROPIC,\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_prompt,\n",
    "    messages=[{\"role\":\"user\",\"content\":user_prompt}]\n",
    ")\n",
    "\n",
    "with claude_message as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end=\"\",flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd3bfc-8755-4ce6-8311-751ca4619e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ollama Streamming\n",
    "result_ollama = ollama.chat(model=MODEL_LLAMA,messages=message, stream= True)\n",
    "for chunk in result_ollama:\n",
    "     # end='' prevents extra newlines\n",
    "    print(chunk[\"message\"][\"content\"],end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950de78-d331-4801-a1ed-9814c8999ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gemini Streamming\n",
    "#Google recently announced endpoints compatible with OpenAI's python client library\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "stream = gemini_via_openai_client.chat.completions.create(\n",
    "    model=MODEL_GEMINI,\n",
    "    messages=message,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content,end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654efc8b-1a04-4c2d-84ef-7d6aa0f96ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922c24a-a33b-4347-bfe7-654608feb78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
