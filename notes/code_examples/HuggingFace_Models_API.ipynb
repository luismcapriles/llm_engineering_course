{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKs1PM-O-VQa"
   },
   "source": [
    "# Models\n",
    "\n",
    "Looking at the lower level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n",
    "\n",
    "This notebook can run on a low-cost or free T4 runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NthhKJRwX1iS",
    "outputId": "1b5b36a8-e0de-44dd-c29d-6f86ebdc7c84"
   },
   "outputs": [],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C9zvDGWD5pKp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "GPU device name: NVIDIA RTX A2000 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 GB allocated\n",
      "0.0 GB reserved\n"
     ]
    }
   ],
   "source": [
    "#to know memory allocation before loading the model to memory\n",
    "print(torch.cuda.memory_allocated()/1e9, \"GB allocated\")\n",
    "print(torch.cuda.memory_reserved()/1e9, \"GB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xd7cEDUC6Lkq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f272eedc694f50a5d2cc8b0972fa21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#login into HuggingFacce \n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Estimate Memory Usage\n",
    "\n",
    "## Pythia model in FP32\n",
    "\n",
    "- **How many parameters?**  400 millions \n",
    "  - \\( 400 * 10^6 \\) parameters  \n",
    "  - 32 bits = 4 bytes  \n",
    "  - (8 bits → 1 byte)  \n",
    "\n",
    "- **How much memory?**  \n",
    "  - \\( 400 * 10^6 \\) parameters × 4 bytes  \n",
    "  - = \\( 1,600 * 10^6 \\) bytes  \n",
    "  - = 1,600 megabytes  \n",
    "  - = 1.6 gigabytes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized models are easier to distribute\n",
    "\n",
    "## 70B parameter model (e.g. Llama 2 70B)\n",
    "- **280 GB storage** in FP32 (32-bit precision)  \n",
    "- Reduce to **40GB** if stored in 4-bit precision  \n",
    "- **7x reduction** (280GB / 40GB) ~ 7  \n",
    "\n",
    "## Llama 2 7B\n",
    "- **28 GB storage** in FP32 (32-bit precision)  \n",
    "- Reduce to **~4GB** if stored in 4-bit precision, in **\"GGUF\"** format.  \n",
    "- **Can run on your computer!**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UtN7OKILQato"
   },
   "outputs": [],
   "source": [
    "# instruct models Text Generation\n",
    "\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "GEMMA2 = \"google/gemma-2-2b-it\"\n",
    "#QWEN2 = \"Qwen/Qwen2-7B-Instruct\" # exercise for you -> too large\n",
    "QWEN2 =\"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # If this doesn't fit it your GPU memory, try others from the hub\n",
    "DEEPSEEKR1 = \"deepseek-ai/DeepSeek-R1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KgxCLBJIT5Hx"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiYqPn87msu"
   },
   "source": [
    "# Accessing Llama 3.1 from Meta\n",
    "\n",
    "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
    "\n",
    "Visit their model instructions page in Hugging Face:\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
    "\n",
    "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
    "\n",
    "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default the model without Quantization is loaded with its Weight at full-precision at FP32 (32 floating points) that takes a lot of memory\n",
    "#model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map={\"\":0}).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #to know memory allocation before loading the model to memory\n",
    "# print(torch.cuda.memory_allocated()/1e9, \"GB allocated\")\n",
    "# print(torch.cuda.memory_reserved()/1e9, \"GB reserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bitsandbytes\n",
    "\n",
    "-Is the easiest option for quantizing a model to 8 and 4-bit\n",
    "\n",
    "-8-bit quantization multiplies outliers in fp16 with non-outliers in int8, converts the non-outlier values back to fp16, and then adds them together to return the weights in fp16. This reduces the degradative effect outlier values have on a model’s performance. \n",
    "\n",
    "-4-bit quantization compresses a model even further, and it is commonly used with QLoRA to finetune quantized LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quantization to BF16 (here we are downcasting from FP32 to BF16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hhOgL1p_T6-b"
   },
   "outputs": [],
   "source": [
    "quant_config_bf16 = BitsAndBytesConfig(bnb_4bit_compute_dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quantization to 8-bits (here we are downcasting from FP32 to 8-bit )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config_8bits = BitsAndBytesConfig(load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quantization to 4-bits (here we are downcasting from FP32 to 4-bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config_4bits = BitsAndBytesConfig(load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the model loaded into memory with quantization\n",
    "#NOTE: device_map=\"auto\" does not work\n",
    "\n",
    "#model_16bits= AutoModelForCausalLM.from_pretrained(LLAMA,device_map={\"\":0},quantization_config=quant_config_bf16)\n",
    "\n",
    "#model_8bit = AutoModelForCausalLM.from_pretrained(LLAMA, device_map={\"\":0},quantization_config=quant_config_8bits)\n",
    "\n",
    "#model_4bit = AutoModelForCausalLM.from_pretrained(LLAMA, device_map={\"\":0},quantization_config=quant_config_4bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 GB allocated\n",
      "0.0 GB reserved\n"
     ]
    }
   ],
   "source": [
    "#to know memory allocation after loading the model with quantization\n",
    "print(torch.cuda.memory_allocated()/1e9, \"GB allocated\")\n",
    "print(torch.cuda.memory_reserved()/1e9, \"GB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check the model memory occupation footprint\n",
    "#memory = model_16bits.get_memory_footprint() / 1e6\n",
    "#print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MtWZYZG920F"
   },
   "source": [
    "If the next cell gives you a 403 permissions error, then please check:\n",
    "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
    "2. Did you set up your API key with full read and write permissions?\n",
    "3. If you visit the Llama3.1 page at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B, does it show that you have access to the model near the top?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399,
     "referenced_widgets": [
      "c5dcd7c557c04463aa1f102f2d542d98",
      "79c5875a76164d839f2a0a31b6ec2f42",
      "58004db10c524eb58e1f0459157aa164",
      "0708d3fecc794f8bb97297ff64fbf80c",
      "f3082af908c442e9901b5bb28f052b8d",
      "7cc43502e7f8426ea2177ff02b6d5e54",
      "434f8ec1b04643f99544afa70d27bcad",
      "3e4e08f5dd6a4da2b99cb17093fdc7ee",
      "ec61527c6d814a2487b0474d947a1824",
      "71b068e2c73a43c8bef08035fd3e8d2e",
      "bc981693fc6c4a25956a94dc2aa11f8a",
      "41f8c4253949445781fa3d4323f49260",
      "8bcdfed460dc4cafafcb55f614a8762c",
      "4e6daa2840f54bc08c031202285ff1d3",
      "9e8ae342a42e4ebe9390d9d577d19ae0",
      "c1f14513d2394266a1f02b95592f21fa",
      "c4205b70ccfd444eabd0d9b6d6c4bbdd",
      "3cecdc2000a046cfa5ad0184b58587d7",
      "17cc72e39876467e85fb86c4f01d0703",
      "a481551ebdc746569ba806446c3a00cd",
      "b931ee41521841c4967e3eb3b75740a2",
      "74cfb00bd1ed4ab69bab43d9734da792",
      "d6090456823e4405a479933907e78396",
      "1ca17e06fbfb4fa7b2e50b6188415b71",
      "9bb15b406eb34b48bdcd0457a2b3dd3d",
      "e7adbbfa12d242aab8a38b7d58584fbc",
      "65f7511de2ae44c19298c0367fd1475f",
      "69698306a62742a693acfb85da063e0d",
      "f17ec563c55f4b4e9956a00b53f27068",
      "a57be6f06cd84634a3ecc4ac2fe06245",
      "b879584a2a9546ddbb35b8b87fec8aee",
      "0c0f6c52ae7b415a9fd548990a7b3138",
      "b37568486d4c44ec8a82bfbb0444a0c5"
     ]
    },
    "id": "Zi8YXiwJHF59",
    "outputId": "14e3eb88-9917-4474-a431-3404005ba6f9"
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SkYEXzbotcud"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistantuser\n",
      "\n",
      "Tell a light-hearted joke for a room of Data Scientistsassistant\n",
      "\n",
      "Why did the linear regression model break up with the decision tree?\n",
      "\n",
      "Because it couldn't handle the variable relationship and the tree's tendency to over-split!\n"
     ]
    }
   ],
   "source": [
    "#Generate the output\n",
    "\n",
    "#this output gives token only\n",
    "outputs = model_16bits.generate(inputs, max_new_tokens=80)\n",
    "\n",
    "#we need to decode the output so we can understand\n",
    "print(tokenizer.decode(outputs[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistantuser\n",
      "\n",
      "Tell a light-hearted joke for a room of Data Scientistsassistant\n",
      "\n",
      "Why did the linear regression go to therapy?\n",
      "\n",
      "Because it was struggling to model its emotions!\n"
     ]
    }
   ],
   "source": [
    "# Create an attention mask that is 0 for pad tokens and 1 for real tokens\n",
    "attention_mask = (inputs != tokenizer.pad_token_id).long().to(\"cuda\")\n",
    " \n",
    "outputs = model_16bits.generate(\n",
    "    inputs,\n",
    "    attention_mask=attention_mask,  # This mask tells the model which tokens to attend to\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Explicitly set pad token\n",
    "    max_new_tokens=80,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2oL0RWU2ttZf"
   },
   "outputs": [],
   "source": [
    "# NOTE Clean up: restart kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDCeJ20e4Hxx"
   },
   "source": [
    "## A couple of quick notes on the next block of code:\n",
    "\n",
    "I'm using a HuggingFace utility called TextStreamer so that results stream back.\n",
    "To stream results, we simply replace:  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80)`  \n",
    "With:  \n",
    "`streamer = TextStreamer(tokenizer)`  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n",
    "\n",
    "Also I've added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts\n",
    "\n",
    "Thank you to student Piotr B for raising the issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernel restarted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RO_VYZ3DZ7cs"
   },
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "\n",
    "def generate(model, messages,quant_config):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model, device_map={\"\":0}, quantization_config=quant_config).to(\"cuda\") #forcing all to the GPU  \n",
    "    outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer,)\n",
    "    \n",
    "    del tokenizer, streamer, model, inputs, outputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "RFjaY4Pdvbfy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1e647befd84169bc08007db76ce119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful assistant<|end|><|user|> Tell a light-hearted joke for a room of Data Scientists<|end|><|assistant|> Sure, here's a light-hearted joke for a room of Data Scientists:\n",
      "\n",
      "Why did the data scientist break up with the algorithm?\n",
      "\n",
      "Because it was always trying to predict their every move and couldn't handle the uncertainty of their love life!<|end|>\n",
      "\n",
      "Filtered output: You are a helpful assistant Tell a light-hearted joke for a room of Data Scientists Sure, here's a light-hearted joke for a room of Data Scientists:\n",
      "\n",
      "Why did the data scientist break up with the algorithm?\n",
      "\n",
      "Because it was always trying to predict their every move and couldn't handle the uncertainty of their love life!\n"
     ]
    }
   ],
   "source": [
    "generate(PHI3, messages,quant_config_bf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxZQmZDCe4Jf"
   },
   "source": [
    "## Accessing Gemma from Google\n",
    "\n",
    "A student let me know (thank you, Alex K!) that Google also now requires you to accept their terms in HuggingFace before you use Gemma.\n",
    "\n",
    "Please visit their model page at this link and confirm you're OK with their terms, so that you're granted access.\n",
    "\n",
    "https://huggingface.co/google/gemma-2-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "q1JW41D-viGy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8639e5ba1204960a9b5a557a2d5f41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Tell a light-hearted joke for a room of Data Scientists<end_of_turn>\n",
      "<start_of_turn>model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luis\\anaconda3\\envs\\llms\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work? \n",
      "\n",
      "Because they heard the data was going to be *high*! 🪜📈 \n",
      "\n",
      "Let me know if you'd like another one! 😄 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]\n",
    "generate(GEMMA2, messages, quant_config_4bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "\n",
    "def generate(model, messages, quant_config):\n",
    "    # torch.cuda.empty_cache()  # Free up GPU memory\n",
    "    # torch.cuda.ipc_collect()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    #to make it work with 8bit quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(model, device_map={\"\":0}, quantization_config=quant_config)\n",
    "    #for the rest of quantization it works with .to() function\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model, device_map={\"\":0}, quantization_config=quant_config).to(\"cuda\")  \n",
    "    \n",
    "    outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "    \n",
    "    del tokenizer, streamer, model, inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell a light-hearted joke for a room of Data Scientists<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sure! Here's one:\n",
      "\n",
      "Why did the data scientist break up with the linear regression?\n",
      "\n",
      "Because he found an outlier in the dataset and couldn't get past it!<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "generate(QWEN2, messages, quant_config_bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell a light-hearted joke for a room of Data Scientists<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sure! Here's a light-hearted joke that might be enjoyed by a room full of data scientists:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "This joke plays on the word \"atoms,\" which is often used to mean \"everything\" in scientific contexts. It's a playful way to poke fun at how we sometimes use words without fully understanding their meanings or implications.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "generate(QWEN2, messages, quant_config_8bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0708d3fecc794f8bb97297ff64fbf80c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71b068e2c73a43c8bef08035fd3e8d2e",
      "placeholder": "​",
      "style": "IPY_MODEL_bc981693fc6c4a25956a94dc2aa11f8a",
      "value": " 55.4k/55.4k [00:00&lt;00:00, 3.27MB/s]"
     }
    },
    "0c0f6c52ae7b415a9fd548990a7b3138": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17cc72e39876467e85fb86c4f01d0703": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ca17e06fbfb4fa7b2e50b6188415b71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69698306a62742a693acfb85da063e0d",
      "placeholder": "​",
      "style": "IPY_MODEL_f17ec563c55f4b4e9956a00b53f27068",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "3cecdc2000a046cfa5ad0184b58587d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e4e08f5dd6a4da2b99cb17093fdc7ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41f8c4253949445781fa3d4323f49260": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8bcdfed460dc4cafafcb55f614a8762c",
       "IPY_MODEL_4e6daa2840f54bc08c031202285ff1d3",
       "IPY_MODEL_9e8ae342a42e4ebe9390d9d577d19ae0"
      ],
      "layout": "IPY_MODEL_c1f14513d2394266a1f02b95592f21fa"
     }
    },
    "434f8ec1b04643f99544afa70d27bcad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e6daa2840f54bc08c031202285ff1d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17cc72e39876467e85fb86c4f01d0703",
      "max": 9085657,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a481551ebdc746569ba806446c3a00cd",
      "value": 9085657
     }
    },
    "58004db10c524eb58e1f0459157aa164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e4e08f5dd6a4da2b99cb17093fdc7ee",
      "max": 55351,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec61527c6d814a2487b0474d947a1824",
      "value": 55351
     }
    },
    "65f7511de2ae44c19298c0367fd1475f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69698306a62742a693acfb85da063e0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71b068e2c73a43c8bef08035fd3e8d2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74cfb00bd1ed4ab69bab43d9734da792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79c5875a76164d839f2a0a31b6ec2f42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cc43502e7f8426ea2177ff02b6d5e54",
      "placeholder": "​",
      "style": "IPY_MODEL_434f8ec1b04643f99544afa70d27bcad",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "7cc43502e7f8426ea2177ff02b6d5e54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bcdfed460dc4cafafcb55f614a8762c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4205b70ccfd444eabd0d9b6d6c4bbdd",
      "placeholder": "​",
      "style": "IPY_MODEL_3cecdc2000a046cfa5ad0184b58587d7",
      "value": "tokenizer.json: 100%"
     }
    },
    "9bb15b406eb34b48bdcd0457a2b3dd3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a57be6f06cd84634a3ecc4ac2fe06245",
      "max": 296,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b879584a2a9546ddbb35b8b87fec8aee",
      "value": 296
     }
    },
    "9e8ae342a42e4ebe9390d9d577d19ae0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b931ee41521841c4967e3eb3b75740a2",
      "placeholder": "​",
      "style": "IPY_MODEL_74cfb00bd1ed4ab69bab43d9734da792",
      "value": " 9.09M/9.09M [00:00&lt;00:00, 24.3MB/s]"
     }
    },
    "a481551ebdc746569ba806446c3a00cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a57be6f06cd84634a3ecc4ac2fe06245": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b37568486d4c44ec8a82bfbb0444a0c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b879584a2a9546ddbb35b8b87fec8aee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b931ee41521841c4967e3eb3b75740a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc981693fc6c4a25956a94dc2aa11f8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1f14513d2394266a1f02b95592f21fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4205b70ccfd444eabd0d9b6d6c4bbdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5dcd7c557c04463aa1f102f2d542d98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_79c5875a76164d839f2a0a31b6ec2f42",
       "IPY_MODEL_58004db10c524eb58e1f0459157aa164",
       "IPY_MODEL_0708d3fecc794f8bb97297ff64fbf80c"
      ],
      "layout": "IPY_MODEL_f3082af908c442e9901b5bb28f052b8d"
     }
    },
    "d6090456823e4405a479933907e78396": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1ca17e06fbfb4fa7b2e50b6188415b71",
       "IPY_MODEL_9bb15b406eb34b48bdcd0457a2b3dd3d",
       "IPY_MODEL_e7adbbfa12d242aab8a38b7d58584fbc"
      ],
      "layout": "IPY_MODEL_65f7511de2ae44c19298c0367fd1475f"
     }
    },
    "e7adbbfa12d242aab8a38b7d58584fbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c0f6c52ae7b415a9fd548990a7b3138",
      "placeholder": "​",
      "style": "IPY_MODEL_b37568486d4c44ec8a82bfbb0444a0c5",
      "value": " 296/296 [00:00&lt;00:00, 10.5kB/s]"
     }
    },
    "ec61527c6d814a2487b0474d947a1824": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f17ec563c55f4b4e9956a00b53f27068": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3082af908c442e9901b5bb28f052b8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
