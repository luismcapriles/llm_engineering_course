{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de946b5-61bf-4e59-89ac-b778a43df5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers accelerate gradio sentencepiece yt-dlp datasets[audio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe01d18-188b-4562-8f43-0d346aa0015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integrating GRADIO to Youtube Video Summary\n",
    "#we will use the Whisper-1 Model for audio to text + Groq\n",
    "#we will use the \"meta-llama/Meta-Llama-3.1-8B-8192\" for summary task\n",
    "#Finally integrate this in Gradio UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba51cf-035d-4d17-b274-38a0dc9c2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import gradio as gr\n",
    "import torch\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import numpy as np\n",
    "import re\n",
    "import yt_dlp\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879a1c9-4fb3-49c3-a75f-eeeea33b028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sign in HF\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05d5d8-2dd3-496e-bf84-f082377e636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groq API\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "groq=Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acba813-75b9-4820-9951-e78e98037395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speech transcription using open-source model:  automatic speech recognition (ASR) ==>> https://huggingface.co/openai/whisper-large-v3-turbo for more info\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1c00c-63fe-45f9-8c41-e0ee7f3260bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Audio from Ytube url\n",
    "def download_audio(url):\n",
    "\n",
    "  ydl_opts = {\n",
    "      'format': 'bestaudio/best',\n",
    "      'postprocessors': [{\n",
    "          'key': 'FFmpegExtractAudio',\n",
    "          'preferredcodec': 'mp3',\n",
    "      }],\n",
    "      'outtmpl': 'audio_file.%(ext)s'  # This sets the output filename\n",
    "  }\n",
    "\n",
    "  with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "      ydl.download([url])\n",
    "\n",
    "      return 'audio_file.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed25b29-2bc0-4d21-9fc0-81084d404de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To handle large audio files >30 min we need a different approach\n",
    "#we need to split the audio in smaller files and process each chunk separately\n",
    "from pydub import AudioSegment #handle audio splitting\n",
    "import math \n",
    "import tempfile\n",
    "\n",
    "def split_audio_file(audio_file_path,chunk_length_ms=10*60*1000): #10-minute chunks\n",
    "    #\"\"Split the audio files into chunks of specified lenght\"\"\n",
    "    audio = AudioSegment.from_mp3(audio_file_path)\n",
    "    chunks = []\n",
    "    \n",
    "    #Calculate how many chunks we need\n",
    "    total_length_ms = len(audio)\n",
    "    num_chunks = math.ceil(total_length_ms/chunk_length_ms) #round-up numbers \n",
    "\n",
    "    #Create a temporary directory to store chunks\n",
    "    #temp_dir = temp_dir.mkdtemp()\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    print(f\"Created temporary directory: {temp_dir}\")\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_ms= i*chunk_length_ms\n",
    "        end_ms=min((i+1)* chunk_length_ms , total_length_ms) # handle the remaining portion of the audio\n",
    "\n",
    "        chunk = audio[start_ms:end_ms]\n",
    "        chunk_path = os.path.join(temp_dir,f\"chunk_{i}.mp3\")\n",
    "        #chunk_export = (chunk_path, format==\"mp3\")\n",
    "        chunk.export(chunk_path, format=\"mp3\")\n",
    "        chunks.append(chunk_path)\n",
    "    return chunks,temp_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2c738-275a-498b-bb2e-9f6d17f197da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to use Groq's API for transcription for chunks files\n",
    "def groq_transcribe_chunk(audio_chunk_path):\n",
    "    url = \"https://api.groq.com/openai/v1/audio/transcriptions\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {groq_api_key}\"\n",
    "    }\n",
    "\n",
    "    with open(audio_chunk_path, \"rb\") as audio_file:\n",
    "        files={\n",
    "            \"file\":(\"audio.mp3\",audio_file,\"audio/mpeg\")\n",
    "        }\n",
    "        data ={\n",
    "            \"model\":\"whisper-large-v3-turbo\",\n",
    "            \"response_format\":\"text\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, headers=headers, files=files, data=data)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            raise Exception(f\"Groq API Error: {response.status_code} - {response.text}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3623bf-84d0-43db-8c13-d592e2d8d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transcribe and summarize actions\n",
    "def transcribe(url):\n",
    "    try:   \n",
    "        #Step 1: Download audio and transcribe using Groq\n",
    "        print(\"Downloading audio from YouTube...\")\n",
    "        audio_path=download_audio(url)\n",
    "\n",
    "        #Step 2: Split the audio into chunks\n",
    "        print(\"Splitting audio into chunks...\")\n",
    "        #chunks = split_audio_file(audio_path)\n",
    "        chunks,temp_dir = split_audio_file(audio_path)\n",
    "\n",
    "        #Step 3: Transcribe each chunk\n",
    "        print(f\"Transcribing {len(chunks)} chunks...\")\n",
    "\n",
    "        full_transcript =\"\"\n",
    "\n",
    "        for i, chunk_path in enumerate(chunks):\n",
    "            print(f\"Transcribing chunk {i+1}/{len(chunks)}...\")\n",
    "            chunk_transcript = groq_transcribe_chunk(chunk_path)\n",
    "            full_transcript += chunk_transcript + \" \"\n",
    "\n",
    "            #Delete the chunk file after processing\n",
    "            os.remove(chunk_path)\n",
    "\n",
    "        #Clean up - remove the temp directory and the original audio file\n",
    "        #os.rmdir(os.path.dirname(chunks[0])) #remove temp directory\n",
    "        os.rmdir(temp_dir)  # Remove temp directory\n",
    "        os.remove(audio_path) #remove original audio file\n",
    "\n",
    "        print(f\"*** Full transcript here: {full_transcript.strip()}\")\n",
    "\n",
    "        #Step 4:Summarize\n",
    "        system_prompt = \"\"\"\n",
    "        \n",
    "        You are a specialized summarization assistant designed to create accurate, informative summaries from audio transcripts. Your primary goal is to extract and organize key information without adding, distorting, or fabricating any content.\n",
    "        \n",
    "        ## Core Guidelines\n",
    "        \n",
    "        1. **Only use information explicitly present in the transcript**. Do not add interpretations, assumptions, or information not directly stated.\n",
    "        2. **Maintain factual accuracy** at all times. If something is ambiguous or unclear in the transcript, acknowledge the uncertainty rather than making assumptions.\n",
    "        3. **Preserve the original meaning and intent** of the speakers.\n",
    "        4. **Use direct quotes** when appropriate to maintain accuracy.\n",
    "        \n",
    "        ## Required Summary Structure\n",
    "        \n",
    "        Your summary must include the following sections in this order:\n",
    "        \n",
    "        1. **Title**: Extract or derive the title directly from the transcript. If no explicit title is mentioned, create a concise, descriptive title based solely on the main topic discussed.\n",
    "        \n",
    "        2. **Main Topic**: A 1-2 sentence description of the central subject being discussed.\n",
    "        \n",
    "        3. **Participants**: Only include this section if speakers are clearly identified in the transcript. List all participants mentioned by name or role.\n",
    "        \n",
    "        4. **Discussion Points**: Outline the key topics covered in the conversation in chronological order. Use bullet points for clarity.\n",
    "        \n",
    "        5. **Highlights**: List 3-5 notable moments, quotes, or insights from the transcript. These should be direct references to content in the transcript.\n",
    "        \n",
    "        6. **Action Points**: Only include this section if specific actions, tasks, or next steps are mentioned in the transcript. List each action item with any associated responsibility or deadline if mentioned.\n",
    "        \n",
    "        7. **Key Takeaways**: Summarize 3-5 main conclusions or important insights from the discussion. These must be directly derived from the transcript content.\n",
    "        \n",
    "        ## Error Prevention Protocol\n",
    "        \n",
    "        - If information for any required section is not present in the transcript, explicitly state \"No [section name] mentioned in the transcript\" rather than fabricating content.\n",
    "        - If uncertain about any information, indicate this with phrases like \"possibly\" or \"appears to be\" rather than stating as fact.\n",
    "        - Double-check all names, numbers, dates, and technical terms against the transcript.\n",
    "        \n",
    "        \n",
    "        ``\n",
    "        Remember: Your primary responsibility is to maintain the integrity of the original content. When in doubt, prioritize accuracy over comprehensiveness.\n",
    "        \"\"\"\n",
    "           \n",
    "        user_prompt = f\"Below is an extract transcript of youtube video. Write summary in markdown of the whole text: \\n{full_transcript.strip()}\"\n",
    "    \n",
    "       \n",
    "         # Create the chat completion using Groq's API\n",
    "        chat_completion = groq.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            messages =[\n",
    "            {\"role\":\"system\", \"content\":system_prompt},\n",
    "            {\"role\":\"user\", \"content\":user_prompt}\n",
    "        ],\n",
    "        max_tokens=8000     #this depends on the model                              \n",
    "        )\n",
    "\n",
    "        # Extract the assistant's response\n",
    "        assistant_response = chat_completion.choices[0].message.content\n",
    "    \n",
    "        return assistant_response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745ef7a-6ef8-4de3-ad71-c8bddd25b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Gradio GUI\n",
    "demo = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Textbox(\n",
    "        label = \"Youtube URL\",\n",
    "        placeholder = \"Enter Youtube video URL here..\",\n",
    "        lines=1\n",
    "    ),\n",
    "\n",
    "    outputs=gr.Markdown(label=\"Video Summary\", min_height=60),\n",
    "    title=\"YouTube video summary\",\n",
    "    description=\"Enter a YouTube URL to get the summary. This process may take a few minutes.\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "demo.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda5dc4-7294-43cb-b411-cf99d1c7c907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
